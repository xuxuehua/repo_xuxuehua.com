---
title: "machine_learning"
date: 2019-07-30 08:38
---
[TOC]



# Machine Learning

所谓的人工智能，在技术层面很多时候就是指机器学习，通过选择特定的算法对样本数据进行计算，获得一个计算模型，并利用这个模型，对以前未曾见过的数据进行预测。如果这个预测在一定程度上和事实相符，我们就认为机器像人一样具有某种智能，即人工智能。

这个过程和人类的学习成长非常类似，也是经历一些事情（获得样本数据），进行分析总结（寻找算法），产生经验（产生模型），然后利用经验（模型）指导自己的日常行为。



机器学习的完整过程也是如此，利用样本数据经过算法训练得到模型，这个模型会和预测系统部署在一起，当外部需要预测的数据到达预测系统的时候，预测系统调用模型，就可以立即计算出预测结果。

因此，构建一个机器学习系统，需要有三个关键要素：样本、模型、算法。



大数据是高效处理海里数据的解决方案。机器学习是基于统计学等数学原理，通过机器计算逻辑，实现识别、分类、预测等目的的算法。人工智能是通过大数据技术，在工程上把机器学习算法变成现实，进而让机器具备类似人类智慧，某些方面甚至远高于人类智慧的复杂认知能力。



## 样本

样本就是通常我们常说的“训练数据”，包括输入和结果两部分。比如我们要做一个自动化新闻分类的机器学习系统，对于采集的每一篇新闻，能够自动发送到对应新闻分类频道里面，比如体育、军事、财经等。这时候我们就需要批量的新闻和其对应的分类类别作为训练数据。通常随机选取一批现成的新闻素材就可以，但是分类需要人手工进行标注，也就是需要有人阅读每篇新闻，根据其内容打上对应的分类标签。

数学上，样本通常表示为：

$$T = (x_{1},y_{1}),(x_{2},y_{2}),…,(x_{n},y_{n})$$

其中$x_{n}$表示一个输入，比如一篇新闻；$y_{n}$表示一个结果，比如这篇新闻对应的类别。

样本的数量和质量对机器学习的效果至关重要，如果样本量太少，或者样本分布不均衡，对训练出来的模型就有很大的影响。就像一个人一样，见得世面少、读书也少，就更容易产生偏见和刻板印象。



## 模型

模型就是映射样本输入与样本结果的函数，可能是一个条件概率分布，也可能是一个决策函数。一个具体的机器学习系统所有可能的函数构成了模型的假设空间，数学表示是：

$$F = {f  | Y = f(X)}$$

其中X是样本输入，$Y$是样本输出，$f$就是建立$X$和$Y$映射关系的函数。所有$f$的可能结果构成了模型的假设空间$F$。

很多时候$F$的函数类型是明确的，需要计算的是函数的参数，比如确定$f$函数为一个线性函数，那么f的函数表示就可以写为：

$$y = a_{1}x + a_{0}$$

这时候需要计算的就是$a_{1}$和$a_{0}$两个参数的值。这种情况下模型的假设空间的数学表示是：

$$F = \left\{f | Y = f_{\theta}(X),\theta\in R^{n} \right\}$$

其中$\theta$为$f$函数的参数取值空间，一个$n$维欧氏空间，被称作参数空间。



## 算法

算法就是要从模型的假设空间中寻找一个最优的函数，使得样本空间的输入$X$经过该函数的映射得到的$f(X)$，和真实的$Y$值之间的距离最小。这个最优的函数通常没办法直接计算得到，即没有解析解，需要用数值计算的方法不断迭代求解。因此如何寻找到$f$函数的全局最优解，以及使寻找过程尽量高效，就构成了机器学习的算法。

如何保证$f$函数或者$f$函数的参数空间最接近最优解，就是算法的策略。机器学习中用损失函数来评估模型是否最接近最优解。损失函数用来计算模型预测值与真实值的差距，常用的有0-1损失函数、平方损失函数、绝对损失函数、对数损失函数等。以平方损失函数为例，损失函数如下：

$$L(Y,f(X)) = (Y-f(X))^{2}$$

对于一个给定的样本数据集

$$T= \left\{ (x_{1},y_{1}),(x_{2},y_{2}),…,(x_{n},y_{n}) \right\}$$

模型$f(X)$相对于真实值的平均损失为每个样本的损失函数的求和平均值：

$$R_{emp}(f)=\frac{1}{N}\sum_{i=1}^{N}{L(y_{i},f(x_{i}))}$$

这个值被称作经验风险，如果样本量足够大，那么使经验风险最小的$f$函数就是模型的最优解，即求

$$\min_{f \in F}{\frac{1}{N}\sum_{i=1}^{N}{L(y_{i},f(x_{i}))}}$$

但是相对于样本空间的可能取值范围，实际中使用的样本量总是有限的，可能会出现使样本经验风险最小的模型$f$函数并不能使实际预测值的损失函数最小，这种情况被称作**过拟合**，即一味追求经验风险最小，而使模型$f$函数变得过于复杂，偏离了最优解。这种情况下，需要引入结构风险以防止过拟合。结构风险表示为：

$$R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}{L(y_{i},f(x_{i}))+\lambda J(f)}$$

在经验风险的基础上加上$\lambda J(f)$，其中$J(f)$表示模型$f$的复杂度，模型越复杂，$J(f)$越大。要使结构风险最小，就要使经验风险和模型复杂度同时小。求解模型最优解就变成求解结构风险最小值：

$$\min_{f \in F}{\frac{1}{N}\sum_{i=1}^{N}{L(y_{i},f(x_{i}))+\lambda J(f)}}$$



一个机器学习模型的参数可能有数百万，训练的样本数据则会更多，因此机器学习通常依赖大数据技术进行模型训练，而机器学习及其高阶形态的神经网络、深度学习则是实现人工智能的主要手段。



## 偏微分方程

机器学习跟偏微分方程究竟是个什么关系？

事实上，关系很简单。机器学习要从假设空间寻找最优函数，而最优函数就是使样本数据的函数值和真实值距离最小的那个函数。给定函数模型，求最优函数就是求函数的参数值。给定不同参数，得到不同函数值和真实值的距离，这个距离就是损失，损失函数是关于模型参数的函数，距离越小，损失越小。最小损失值对应的函数参数就是最优函数。

而我们知道，数学上求极小值就是求一阶导数，计算每个参数的一阶导数为零的偏微分方程组，就可以算出最优函数的参数值。这就是为什么机器学习要计算偏微分方程的原因。







## 感知机



从机器学习模型角度看，目前最简单的机器学习模型大概就是感知机了，而最火热的机器学习模型则是神经网络。人工智能领域几乎所有炫酷的东西都是神经网络的成果，有下赢人类最顶尖围棋棋手的AlphaGo、自动驾驶技术、聊天机器人、语音识别与自动翻译等。

事实上，神经网络和感知机是一脉相承的，就像复杂的人体是由一个个细胞组成、复杂的大脑是由一个个神经元组成，而神经网络正是由感知机组成的。



感知机是一种比较简单的二分类模型，将输入特征分类为+1、-1两类，就像下图所示的，一条直线将平面上的两类点分类。

![img](https://snag.gy/aM0KHe.jpg)



二维平面上的点只有两个输入特征（横轴坐标和纵轴坐标），一条直线就可以分类。如果输入数据有更多维度的特征，那么就需要建立同样多维度的模型，高维度上的分类模型也被称为超平面。

感知机模型如下：

$$f(x)=sign(w\cdot x+b)$$

其中$x$代表输入的特征空间向量，输出空间是{-1, +1}，$w$为权值向量，$b$叫作偏置，$sign$是一个符号函数。

![img](https://snag.gy/ykMtIH.jpg)



$w\cdot x+b=0$为超平面的方程，当感知机输出为+1表示输入值在超平面的上方，当感知机输出为-1表示输入值在超平面的下方。训练感知机模型就是要计算出$w$和$b$的值，当有新的数据需要分类的时候，输入感知机模型就可以计算出+1或者-1从而进行分类。

由于输出空间只有{-1, +1}两个值，所以只有误分类的时候，才会有模型计算值和样本真实值之间的偏差，偏差之和就是感知机的损失函数。

$$L(w,b)=-\sum_{x_{i}\in M}{y_i}({w\cdot{x_i}+b})$$

其中$M$为误分类点集合，误分类点越少，损失函数的值越小；如果没有误分类点，损失函数值为0。求模型的参数$w$和$b$，就是求损失函数的极小值。

数学上求函数的极小值就是求函数的一阶导数，但是感知机损失函数用统计求和函数表达，没办法计算解析解。机器学习采用梯度下降法求损失函数极小值，实质上就是求导过程的数值计算方法。

对于误分类点集合$M$，损失函数$L(w,b)$变化的梯度，就是某个函数变量的变化引起的函数值的变化，根据感知机损失函数可知：

$$\Delta_{w}L(w,b)=-\sum_{x_i\in M}{y_i x_i}$$

$$\Delta_{b}L(w,b)=-\sum_{x_i\in M}{y_i}$$

使用梯度下降更新$w$和$b$，不断迭代使损失函数$L(w,b)$不断减小，直到为0，也就是没有误分类点。感知机算法的实现过程：

1.选择初始值$w_0,b_0$。

2.在样本集合中选择样本数据$x_i,y_i$。

3.如果$y_i(w\cdot x_i+b)< 0$，表示$y_i$为误分类点，那么$w = w +\eta y_i x_i$、$b = b +\eta y_i$，在梯度方向校正$w$和$b$。其中$\eta$为步长，步长选择要适当，步长太长会导致每次计算调整太大出现震荡；步长太短又会导致收敛速度慢、计算时间长。

4.跳转回2，直到样本集合中没有误分类点， 即全部样本数据$y_i(w\cdot x_i+b)\geq 0$



## 神经网络

我们现在所说的神经网络，通常是指机器学习所使用的“人工神经网络”，是对人脑神经网络的一种模拟。人脑神经网络由许多神经元构成，每个神经元有多个树突，负责接收其他神经元的输出信号，神经元细胞完成对输入信号的处理，转换成输出信号，通过突触传递给其他神经元。

![img](https://snag.gy/eDzd7x.jpg)





神经元细胞的输出只有0或者1两种输出，但是人脑大约有140亿个神经元，这些神经元组成一个神经网络，前面的神经元输出作为后面的神经元输入进一步处理，最终实现人类的智能。

人脑神经元可以通过感知机进行模拟，每个感知机相当于一个神经元，使用$sign$函数的感知机输出也是只有两个值，跟人脑神经元一样。

![img](https://snag.gy/m7aP3s.jpg)



$x_1,x_2,x_3$相当于神经元的树突，实现信号的输入；$sum()+b$及$sign$函数相当于神经元细胞，完成输入的计算；$y$是神经元的输出，上图用数学形式表达的话是
$$y=sign(w_1x_1+w_2x_2+w_3x_3+b)$$

它是感知机$y=sign(w\cdot x+b)$向量展开形式。

将感知机组成一层或者多层网络状结构，就构成了机器学习神经网络。下图就是一个两层神经网络。



![img](https://snag.gy/zVX0T8.jpg)



在多层神经网络中，每一层都由多个感知机组成。将输入的特征向量$x$传递给第一层的每一个感知机，运算以后作为输出传递给下一层的每一个感知机，直到最后一层感知机产生最终的输出结果。这就是机器学习神经网络的实现过程，通过模拟人脑神经网络，利用样本数据训练每个感知机神经元的参数，在某些场景下得到的模型可以具有不可思议的效果。



以神经网络实现手写数字识别为例，样本如下。

![img](https://snag.gy/VwhNgY.jpg)



这个手写数字样本中的每个数字都是一个28×28像素的图片，我们把每个像素当作一个特征值，这样每个数字就对应784个输入特征。因为输出需要判别10个数字，所以第二层（输出层）的感知机个数就是10个，每个感知机通过0或者1输出是否为对应的数字。

![img](https://snag.gy/8UvK1e.jpg)



使用梯度下降算法，利用样本数据，可以训练神经网络识别手写数字，计算每个感知机的$w$和$b$参数值。当所有的感知机参数都计算出来，神经网络也就训练出来了。这样对于新输入的手写数字图片，可以进行自动识别，输出对应的数字。

训练神经网络的时候采用一种反向传播的算法，针对每个样本，从最后一层，也就是输出层开始，利用样本结果使用梯度下降算法计算每个感知机的参数。然后以这些参数计算出来的结果作为倒数第二层的输出计算该层的参数。然后逐层倒推，反向传播，计算完所有感知机的参数。

当选择两层神经网络的时候，原始感知机的$sign$函数表现并不太好，更常用的是$sigmoid$函数。



![img](https://snag.gy/H2fM4d.jpg)



对于两层以上的多层神经网络，$ReLU$函数的效果更好一些。$ReLU$函数表达式非常简单

$$y=max(x,0)$$

当$x$大于0，输出$x$；当$x$小于0，输出0。

神经网络根据组织和训练方式的不同有很多类型。当神经网络层数比较多的时候，我们称它们为深度学习神经网络。前两年在人工智能领域大放异彩的围棋程序AlphaGo则是一种卷积神经网络。

![img](https://snag.gy/aXTpWw.jpg)



对于一个19×19的围棋棋盘，在下棋过程中，每个位置有黑、白、空三种状态，将其提取为特征就是神经网络的输入（事实上，输入特征还需要包括气、眼、吃等围棋规则盘面信息）。而输出设置19×19即361个感知机产生对应的落子。然后将大量人类的棋谱，即当前盘面下的最佳落子策略作为训练样本，就可以训练出一个智能下棋的神经网络。

但是这样根据人类棋谱训练得到神经网络最多就是人类顶尖高手的水平，AlphaGo之所以能够碾压人类棋手还依赖一种叫蒙特卡洛搜索树的的算法，对每一次落子以后的对弈过程进行搜索，判断出真正的最佳落子策略。利用蒙特卡洛搜索树结合神经网络，AlphaGo还可以进行自我对弈，不断进行自我强化，找到近乎绝对意义上的最优落子策略。















