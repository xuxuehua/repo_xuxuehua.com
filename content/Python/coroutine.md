---
title: "coroutine 协程"
date: 2019-08-15 08:51
---
[TOC]



# coroutine 协程

协程是实现并发编程的一种方式。

协程相对于多线程，其为单线程

是一种可以暂停的函数，可以在暂停的地方传入值

协程需要配合事件循环来使用的



为了减少线程的切换，我们可以创建一个等于 CPU 核数的线程池，把需要运算的逻辑放进线程池，不需要时就拿出来换成其他的任务，保持线程的持续运算而不是切换。

为了更好的使用 CPU 的性能，我们应该在任务不在需要 CPU 资源时让其从线程池里退出来（比如等待 IO 时），这就需要有一种机制，让任务可以在阻塞时退出，在资源就绪时恢复运行。 所以我们将任务抽象为一种用户态的线程（协程，greenthread、coroutine），当其需要调用阻塞资源时，就在 IO 调度器里注册一个事件，并让出线程资源给其他协程， 当资源就绪时，IO 调度器会在有空余线程资源时，重新运行这个协程。



## 优势

协程解决了

1. 回调模式编写难的问题

2. 同步编程的并发性不高

3. 多线程编程需要线程间同步（Lock）的问题

协程的优点在于，这是一种用户态的机制，避免的内核态用户态切换的成本，而且初始栈空间可以设置的很小（Golang 中的 goroutine 仅为 2 KB），这样可以创建比线程更大数量的协程。





```
In [24]: import asyncio 
    ...:  
    ...:  
    ...: async def crawl_page(url): 
    ...:     print('crawling {}'.format(url)) 
    ...:     sleep_time = int(url.split('_')[-1]) 
    ...:     await asyncio.sleep(sleep_time) 
    ...:     print('OK {}'.format(url)) 
    ...:  
    ...:  
    ...: async def main(urls): 
    ...:     for url in urls: 
    ...:         await crawl_page(url) 
    ...:  
    ...: %time asyncio.run(main(['url_1', 'url_2', 'url_3', 'url_4'])) 
    ...:  

>>>
crawling url_1
OK url_1
crawling url_2
OK url_2
crawling url_3
OK url_3
crawling url_4
OK url_4
CPU times: user 2.16 ms, sys: 2.87 ms, total: 5.04 ms
Wall time: 10 s
```



## 协程分类

用户态线程（下文称之为协程）的设计方案一般有三种（按照用户态线程和系统线程的比例）：

- 1:1：直接使用系统线程，可以利用多核，但是上下文开销大；
- N:1：多协程对应一个线程，节省了上下文开销，缺点是不能利用多核，asyncio 就是这个方案；
- M:N：多协程对应多线程，golang 的方案。





## C10M问题

如何利用8核心CPU，64GB内存，在10Gbps的网络上保持1000万的并发连接







## 同步

代码调用IO操作时，必须等待IO操作完成才能返回



## 异步

代码调用IO操作时，不必等待IO操作完成返回



## 阻塞

调用函数时，当前线程被挂起



## 非阻塞

调用函数时，当前线程不会被挂起，而是立即返回







## async 

声明异步函数， 而调用异步函数，便可以得到一个协程对象coroutine object





## await 

可以通过await方法调用协程对象，await 执行的效果，和 Python 正常执行是一样的，也就是说程序会阻塞在这里，进入被调用的协程函数，执行完毕返回后再继续，而这也是 await 的字面意思

await是同步调用







# Unix下I/O 模型

## 阻塞式I/O （常用）

通常我们使用的I/O都是阻塞式I/O，在编程时使用的大多数也是阻塞式I/O。在默认情况下，所有的套接字(socket)都是阻塞的。

浪费时间很严重

![image-20200823121558035](coroutine.assets/image-20200823121558035.png)



**优点**
阻塞式I/O很容易上手，一般程序按照read-process的顺序进行处理就好。通常来说我们编写的第一个TCP的C/S程序就是阻塞式I/O模型的。并且该模型定位错误，在阻塞时整个进程将被挂起，基本不会占用CPU资源。

**缺点**
该模型的缺点也十分明显。作为服务器，需要处理同时多个的套接字，使用该模型对具有多个的客户端并发的场景时就显得力不从心。
当然也有补救方法，我们使用多线程技术来弥补这个缺陷。但是多线程在具有大量连接时，多线程技术带来的资源消耗也不容小看：

> 如果我们现在有1000个连接时，就需要开启1000个线程来处理这些连接，于是就会出现下面的情况
>
> - 线程有内存开销，假设每个线程需要512K的存放栈，那么1000个连接就需要月512M的内存。当并发量高的时候，这样的内存开销是无法接受的。
> - 线程切换有CPU开销，这个CPU开销体现在上下文切换上，如果线程数越多，那么大多数CPU时间都用于上下文切换，这样每个线程的时间槽会非常短，CPU真正处理数据的时间就会少了非常多。







## 非阻塞式I/O

有阻塞I/O，那么也会有非阻塞I/O，在上文说过默认情况下，所有的套接字都是阻塞的，那么通过设置套接字的NONBLOCK(一般在`open()`,`socket()`等调用中设置)标志或者设置`recv`、`send`等输入输出函数的**MSG_DONTWAIT**标志就可以实现非阻塞操作。

![image-20200823121804401](coroutine.assets/image-20200823121804401.png)

> 可以看到，前三次*recvfrom*时没有数据可以返回，此时内核不阻塞进程，转而立即返回一个**EWOULDBLOCK**错误。
>
> 第四次调用*recvfrom*时已经有一个数据报准备好了，此时它将被复制到应用进程的缓冲区，于是*recvfrom*调用成功返回。
> 当一个应用进程像这样对一个非阻塞描述符循环调用*recvfrom*时，我们称之为**轮询(polling)**

**优点**：
这种I/O方式也有明显的优势，即不会阻塞在内核的等待数据过程，每次发起的I/O请求可以立即返回，不用阻塞等待。在数据量收发不均，等待时间随机性极强的情况下比较常用。

**缺点**
轮询这一个特征就已近暴露了这个I/O模型的缺点。轮询将会不断地询问内核，这将占用大量的CPU时间，系统资源利用率较低。同时，该模型也不便于使用，需要编写复杂的代码。



## I/O复用

在出现大量的链接时，使用多线程+阻塞I/O的编程模型会占用大量的内存。那么I/O复用技术在内存占用方面，就有着很好的控制。
当前的高性能反向代理服务器**Nginx**使用的就是I/O复用模型(epoll),它以高性能和低资源消耗著称，在大规模并发上也有着很好的表现。

![image-20200823222847200](coroutine.assets/image-20200823222847200.png)



那到底什么是I/O复用(I/O multiplexing)。根据我的理解，复用指的是复用线程，从阻塞式I/O来看，基本一个套接字就霸占了整个线程。例如当对一个套接字调用*recvfrom*调用时，整个线程将被阻塞挂起，直到数据报准备完毕。
多路复用就是复用一个线程的I/O模型，Linux中拥有几个调用来实现I/O复用的系统调用——`select`,`poll`,`epoll`（Linux 2.6+）

线程将阻塞在上面的三个系统调用中的某一个之上，而不是阻塞在真正的I/O系统调用上。I/O复用允许对多个套接字进行监听，当有某个套接字准备就绪(可读/可写/异常)时，系统调用将会返回。
然后我们可能将重新启用一个线程并调用*recvfrom*来将特定套接字中的数据报从内核缓冲区复制到进程缓冲区。



**优点**
I/O复用技术的优势在于，只需要使用一个线程就可以管理多个socket，系统不需要建立新的进程或者线程，也不必维护这些线程和进程，所以它也是很大程度上减少了资源占用。
另外I/O复用技术还可以同时监听不同协议的套接字

**缺点**
在只处理连接数较小的场合，使用select的服务器不一定比多线程+阻塞I/O模型效率高，可能延迟更大，因为单个连接处理需要2次系统调用，占用时间会有增加。



## 信号驱动式I/O （使用少）

当然你可能会想到使用信号这一机制来避免I/O时线程陷入阻塞状态。那么内核开发者怎么可能会想不到。

![image-20200823223039254](coroutine.assets/image-20200823223039254.png)

> 首先开启套接字的信号驱动式I/O功能，并通过*sigaction*系统调用来安装一个信号处理函数，我们进程不会被阻塞。
> 当数据报准备好读取时，内核就为该进程产生一个**SIGIO**信号，此时我们可以在信号处理函数中调用*recvfrom*读取数据报，并通知数据已经准备好，正在等待处理。



**优点**
很明显，我们的线程并没有在等待数据时被阻塞，可以提高资源的利用率
**缺点**
其实在Unix中，信号是一个被过度设计的机制(这句话来自知乎大神,有待考究)
信号I/O在大量IO操作时可能会因为**信号队列溢出**导致没法通知——这个是一个非常严重的问题。



## 异步I/O 

前面说过这4种I/O模型都可以划分为同步I/O方式，那我们来看看为什么。
了解了４种I/O模型的调用过程后，我们可以注意到，在数据从内核缓冲区复制到用户缓冲区时，都需要进程显示调用*recvfrom*，并且这个复制过程是阻塞的。
也就是说真正I/O过程(这里的I/O有点狭义，指的是内核缓冲区到用户缓冲区)是同步阻塞的，不同的是各个I/O模型在数据报准备好之前的动作不一样。



异步I/O，是由POSIX规范定义的。这个规范定义了一些函数，这些函数的工作机制是：告知内核启动某个操作，并让内核在整个操作完成后再通知我们。(包括将数据从内核复制到我们进程的缓冲区)

POSIX的aio_系列函数， aio函数比较复杂

目前高并发框架并不使用aio，而都是io多路复用技术，成熟并稳定

异步I/O在Linux2.6才引入，而且到现在仍然未成熟。
虽然有知名的异步I/O库 `glibc`，但是听说`glibc`采用多线程模拟，但存在一些bug和设计上的不合理。

![image-20200823223955308](coroutine.assets/image-20200823223955308.png)



## IO多路复用（实际常用）

IO多路复用就是通过一种机制，一个进程可以监视多个描述符（socket），一旦某个描述符就绪（一般是读就绪或者是写就绪），能够通知程序进行相应的读写操作。

但select，poll， epoll本质上都是同步I/O，都需要读写事件就绪之后自己负责读写，即这个自己读写的过程是阻塞的。

而异步I/O（aio系列函数）则无需自己负责进行读写，异步I/O的实现会负责把数据从内核copy到用户空间



### select

select 函数监视的文件描述符为3类， writefsd， readfds，execptfds

调用后select函数会阻塞，直到有描述符就绪（即有数据读写，或者except）或者超时（timeout所指定的等待时间，如果立刻返回设置为null即可），函数返回。

select函数返回后，可以通过遍历所有的fdset找到就绪描述符



在并发不高，同时连接很活跃的情况下，select 比epoll要好



select目前几乎在所有平台上都支持

缺点是单个进程能够监视的文件描述符的数量存在最大限制，Linux一般是1024，可以通过修改宏定义甚至重新编译内核的方式提升限制，但是也会造成效率低下





### poll

不同于select使用三个位图来表示三个fdset的方式，poll使用一个pollfd的指针实现



pollfd结构包含了要监视的event和发生的event，不再适用select的参数-值的传递方式。

pollfd没有最大数量限制（但数量过大后性能也会下降）

和selec函数一样，poll返回后，需要轮询pollfd来获取就绪的socket，但事实上，同时连接的大量客户端再一个时刻只有很少的处于就绪状态，因此随着监视的socket数量增长，效率也会线性下降



### epoll （only Linux)

Linux 2.6 内核中提出，是之前select 和poll的增强版本。

epoll也没有最大socket的数量限制，epoll使用一个文件描述符，管理多个描述符，将用户关系的文件描述符的事件，存放到内核的一个事件表中，这样用户空间和内核空间的copy只需要一次即可

epoll的查询采用红黑树的方式提高效率



在高并发的情况下，连接活跃度不是很高（如网站访问），epoll比select 要好



#### 回调模式（不常用）

不停的请求socket的状态并调用对应的回调函数 (回调函数代码很难维护， 可读性差，异常处理困难)

select本身是不支持register 模式，socket 状态变化后的回调是由程序员完成的

```python
def loop():
    while not stop:
        ready = selector.select()
        for key, mask in ready:
            call_back = key.data
            call_back(key)
```

> 完成回调+事件循环+select(epoll), 只有一个线程，减少了线程切换的IO消耗，减少了内存的使用

