---
title: "replication"
date: 2021-07-07 17:00
---



[toc]



# 主从复制 (amazon linux2)



| **Node Name**   | **IP Address** | **Role**  | **Apps Running**         |
| --------------- | -------------- | --------- | ------------------------ |
| PG-Node1        | 10.10.2.101    | Primary   | PostgreSQL 12 and repmgr |
| PG-Node2        | 10.10.2.102    | Standby 1 | PostgreSQL 12 and repmgr |
| PG-Node3        | 10.10.2.103    | Standby 2 | PostgreSQL 12 and repmgr |
| PG-Node-Witness | 10.10.2.104    | Witness   | PostgreSQL 12 and repmgr |





## master configuration

sudoers

```
echo "%postgres        ALL=(ALL)       NOPASSWD: ALL" >> /etc/sudoers
```



创建角色用于同步

```
-bash-4.2$ /usr/pgsql-12/bin/psql
psql (12.7)
Type "help" for help.

# PostgreSQL replication user
postgres=# CREATE USER replica WITH REPLICATION ENCRYPTED PASSWORD 'replica';
CREATE ROLE

```







Allow replication connections from Standby to Master by appending a similar line as following to the pg_hba.conf file of the Master. If you are enabling automatic failover using any external tool, you must also allow replication connections from Master to the Standby. In the event of a failover, the Standby may be promoted as a Master and the Old Master need to replicate changes from the New Master (previously a standby). You may use any of the [authentication methods as supported by PostgreSQL today](https://www.postgresql.org/docs/12/auth-pg-hba-conf.html).

```
vim /var/lib/pgsql/12/data/pg_hba.conf

host	 replication	replica		10.10.2.0/24(从库ip)	md5
```





配置postgresql.conf

```
vim /var/lib/pgsql/12/data/postgresql.conf 
listen_addresses = '*' 
max_wal_senders = 10
max_replication_slots = 10
wal_level = 'replica'
hot_standby = on
archive_mode = on
archive_command = '/bin/true'


-bash-4.2$ psql -c "select pg_reload_conf()"
 pg_reload_conf 
----------------
 t
(1 row)

[root@ip-172-31-9-108 12]# systemctl restart postgresql-12
```



testing connection from slave to master

```
[root@ip-172-31-57-50 ~]# /usr/pgsql-12/bin/psql -h MASTER_IP -U postgres
psql (12.7)
Type "help" for help.

postgres=# \q
```











## slave configuration

sudoers

```
echo "%postgres        ALL=(ALL)       NOPASSWD: ALL" >> /etc/sudoers
```



基础备份 and then 清空数据文件夹

```
systemctl stop postgresql-12

[root@ip-172-31-57-50 ~]# sudo su - postgres
Last login: Wed Jul  7 10:12:37 UTC 2021 on pts/0
-bash-4.2$ rm -rf /var/lib/pgsql/12/data
-bash-4.2$ 
```



从主节点拷贝数据 * 需要使用postgres用户进行操作否则启动从节点将会有问题

You may use pg_basebackup to backup the data directory of the Master from the Standby. While creating the backup, you may also tell pg_basebackup to create the replication specific files and entries in the data directory using "-R" .

```
-bash-4.2$ pg_basebackup -h MASTER_IP -D /var/lib/pgsql/12/data -p 5432 -U replica -Fp -Xs -Pv -R
Password: 
pg_basebackup: initiating base backup, waiting for checkpoint to complete
pg_basebackup: checkpoint completed
pg_basebackup: write-ahead log start point: 0/3000028 on timeline 1
pg_basebackup: starting background WAL receiver
pg_basebackup: created temporary replication slot "pg_basebackup_30016"
340604/340604 kB (100%), 1/1 tablespace                                         
pg_basebackup: write-ahead log end point: 0/3000138
pg_basebackup: waiting for background process to finish streaming ...
pg_basebackup: syncing data to disk ...
pg_basebackup: base backup completed
```



You may use multiple approaches such as rsync or any other disk backup methods to copy the master’s data directory to the standby. But, there is an important file (standby.signal) that must exist in a standby data directory to help postgres determine its state as a standby. It is automatically created when you use the "-R" option while taking pg_basebackup. If not, you may simply use touch to create this empty file.

```
[root@ip-172-31-57-50 ~]# sudo su - postgres
Last login: Wed Jul  7 10:13:01 UTC 2021 on pts/0
-bash-4.2$ ls -la $PGDATA | grep signal
-rw-------  1 postgres postgres     0 Jul  8 09:31 standby.signal
-bash-4.2$ cat $PGDATA/standby.signal
-bash-4.2$ touch $PGDATA/standby.signal
```



One of the most important observations should be the contents of the postgresql.auto.conf file in the standby server. As you see in the following log, an additional parameter primary_conninfo has been added to this file. This parameter tells the standby about its Master. If you haven’t used pg_basebackup with -R option, you would not see this entry (of primary_conninfo) in this file, on the standby server. Which means that you have to add this manually.

```
-bash-4.2$ cat $PGDATA/postgresql.auto.conf
# Do not edit this file manually!
# It will be overwritten by the ALTER SYSTEM command.
primary_conninfo = 'user=replica password=replica host=MASTER_IP port=5432 sslmode=prefer sslcompression=0 gssencmode=prefer krbsrvname=postgres target_session_attrs=any'
```



```
systemctl restart postgresql-12
```





## repmgr

repmgr is an open-source toolset from 2ndQuadrant, a leading specialist in PostgreSQL-related technologies and services. The product is used to automate, enhance, and manage PostgreSQL streaming replication.



Streaming replication in PostgreSQL has been around since version 9.0. Natively setting up and managing streaming replication involves a number of manual steps which includes:

- Configuring replication parameters in both primary and each standby node
- Backing up primary node data with pg_basebackup from each standby node and restoring it there
- Restarting the standby node(s)

From an operational side, a few tasks include:

- Checking replication status using SQL statements
- Promoting a standby node when a switchover is necessary or when the primary is unavailable
- Fencing off failed or stopped primary node
- Recreating replication from the new read/write node to existing or new standby nodes

With repmgr most of these tasks can be automated, saving the DBA and operational staff both time and effort.



```
sudo yum install -y repmgr12
```



repmgr uses its own database to store its metadata. It is also recommended to use a special PostgreSQL user account for this application. The username and the database name can be anything, but for simplicity, we will call them both “repmgr”. Also, the user will be created as a PostgreSQL superuser. This is recommended by 2ndQuadrant for simplicity’s sake, as some repmgr operations require elevated privileges.

We are running the following commands in the **primary node only**. Note that we have switched to the postgres user from the shell prompt before running these commands.

```
-bash-4.2$ createuser --superuser repmgr
-bash-4.2$ createdb --owner=repmgr repmgr
-bash-4.2$ psql -c "ALTER USER repmgr SET search_path TO repmgr, public;"
ALTER ROLE
-bash-4.2$ echo "shared_preload_libraries = 'repmgr'" >> 12/data/postgresql.conf 
```

This will load the repmgr extension when PostgreSQL starts. By default any PostgreSQL configuration files present in the data directory will be copied when cloning a standby, so any settings configured for the primary will be copied to the standby as well.





### primary node



```
sudo vim /etc/repmgr/12/repmgr.conf
node_id=1
node_name='PG-Node1'
conninfo='host=10.10.2.101 user=repmgr dbname=repmgr connect_timeout=2'
data_directory='/var/lib/pgsql/12/data'
```



### standby node

```
sudo vim /etc/repmgr/12/repmgr.conf
node_id=2
node_name='PG-Node2'
conninfo='host=10.10.2.102 user=repmgr dbname=repmgr connect_timeout=2'
data_directory='/var/lib/pgsql/12/data'
```



```
sudo vim /etc/repmgr/12/repmgr.conf
node_id=3
node_name='PG-Node3'
conninfo='host=10.10.2.103 user=repmgr dbname=repmgr connect_timeout=2'
data_directory='/var/lib/pgsql/12/data'
```



### pg_hba.conf

 add the following lines in the pg_hba.conf file in PG-Node1,the pg_hba.conf file from the primary node will be copied to the two standbys when repmgr sets up replication. Note how we are using the CIDR range of the cluster instead of specifying individual IP addresses.

```
-bash-4.2$ vim 12/data/pg_hba.conf
local   replication     repmgr                              trust
host    replication     repmgr      127.0.0.1/32            trust
host    replication     repmgr      10.10.2.0/24             trust

local   repmgr          repmgr                              trust
host    repmgr          repmgr      127.0.0.1/32            trust
host    repmgr          repmgr      10.10.2.0/24             trust


-bash-4.2$ sudo systemctl restart postgresql-12
```







## verify results

To test if the standby nodes can connect to the primary node, we are running the following command from both PG-Node2 and PG-Node3:

```
psql 'host=10.10.2.101 user=repmgr dbname=repmgr connect_timeout=2'
```





We then run the following command in the primary node (PG-Node1) as the postgres user. This registers the primary node PostgreSQL instance with repmgr. This command installs the repmgr extension. It also adds metadata about the primary node in the repmgr database.

```
-bash-4.2$ /usr/pgsql-12/bin/repmgr -f /etc/repmgr/12/repmgr.conf primary register
INFO: connecting to primary database...
NOTICE: attempting to install extension "repmgr"
NOTICE: "repmgr" extension successfully installed
NOTICE: primary node record (ID: 1) registered


-bash-4.2$ /usr/pgsql-12/bin/repmgr -f /etc/repmgr/12/repmgr.conf cluster show
 ID | Name     | Role    | Status    | Upstream | Location | Priority | Timeline | Connection string                                           
----+----------+---------+-----------+----------+----------+----------+----------+--------------------------------------------------------------
 1  | PG-Node1 | primary | * running |          | default  | 100      | 1        | host=10.10.2.101 user=repmgr dbname=repmgr connect_timeout=2
```





following command in *both* the standby nodes (PG-Node2 and PG-Node3) as the postgres user for a *dry-run* before actually cloning from the primary

```
-bash-4.2$ sudo systemctl stop postgresql-12
-bash-4.2$ rm -rf 12/data/*
-bash-4.2$ /usr/pgsql-12/bin/repmgr -h 10.10.2.101 -U repmgr -d repmgr -f /etc/repmgr/12/repmgr.conf standby clone --dry-run
NOTICE: destination directory "/var/lib/pgsql/12/data" provided
INFO: connecting to source node
DETAIL: connection string is: host=10.10.2.101 user=repmgr dbname=repmgr
DETAIL: current installation size is 32 MB
INFO: "repmgr" extension is installed in database "repmgr"
INFO: replication slot usage not requested;  no replication slot will be set up for this standby
INFO: parameter "max_wal_senders" set to 10
NOTICE: checking for available walsenders on the source node (2 required)
INFO: sufficient walsenders available on the source node
DETAIL: 2 required, 10 available
NOTICE: checking replication connections can be made to the source server (2 required)
INFO: required number of replication connections could be made to the source server
DETAIL: 2 replication connections required
WARNING: data checksums are not enabled and "wal_log_hints" is "off"
DETAIL: pg_rewind requires "wal_log_hints" to be enabled
NOTICE: standby will attach to upstream node 1
HINT: consider using the -c/--fast-checkpoint option
INFO: all prerequisites for "standby clone" are met
```



If both the standby nodes show all prerequisites for standby clone are met, we can go ahead with the clone operation:

```
-bash-4.2$ ls -l 12/data
total 0


-bash-4.2$ /usr/pgsql-12/bin/repmgr -h 10.10.2.101 -U repmgr -d repmgr -f /etc/repmgr/12/repmgr.conf standby clone
NOTICE: destination directory "/var/lib/pgsql/12/data" provided
INFO: connecting to source node
DETAIL: connection string is: host=10.10.2.101 user=repmgr dbname=repmgr
DETAIL: current installation size is 32 MB
INFO: replication slot usage not requested;  no replication slot will be set up for this standby
NOTICE: checking for available walsenders on the source node (2 required)
NOTICE: checking replication connections can be made to the source server (2 required)
WARNING: data checksums are not enabled and "wal_log_hints" is "off"
DETAIL: pg_rewind requires "wal_log_hints" to be enabled
INFO: checking and correcting permissions on existing directory "/var/lib/pgsql/12/data"
NOTICE: starting backup (using pg_basebackup)...
HINT: this may take some time; consider using the -c/--fast-checkpoint option
INFO: executing:
  /usr/pgsql-12/bin/pg_basebackup -l "repmgr base backup"  -D /var/lib/pgsql/12/data -h 10.10.2.101 -p 5432 -U repmgr -X stream 
NOTICE: standby clone (using pg_basebackup) complete
NOTICE: you can now start your PostgreSQL server
HINT: for example: pg_ctl -D /var/lib/pgsql/12/data start
HINT: after starting the server, you need to register this standby with "repmgr standby register"


-bash-4.2$ ls -l 12/data
total 64
-rw------- 1 postgres postgres   217 Jul 11 11:47 backup_label
drwx------ 6 postgres postgres    54 Jul 11 11:47 base
-rw------- 1 postgres postgres    30 Jul 11 11:47 current_logfiles
drwx------ 2 postgres postgres  4096 Jul 11 11:47 global
drwx------ 2 postgres postgres    32 Jul 11 11:47 log
drwx------ 2 postgres postgres     6 Jul 11 11:47 pg_commit_ts
drwx------ 2 postgres postgres     6 Jul 11 11:47 pg_dynshmem
-rw------- 1 postgres postgres  4915 Jul 11 11:47 pg_hba.conf
-rw------- 1 postgres postgres  1636 Jul 11 11:47 pg_ident.conf
drwx------ 4 postgres postgres    68 Jul 11 11:47 pg_logical
drwx------ 4 postgres postgres    36 Jul 11 11:47 pg_multixact
drwx------ 2 postgres postgres     6 Jul 11 11:47 pg_notify
drwx------ 2 postgres postgres     6 Jul 11 11:47 pg_replslot
drwx------ 2 postgres postgres     6 Jul 11 11:47 pg_serial
drwx------ 2 postgres postgres     6 Jul 11 11:47 pg_snapshots
drwx------ 2 postgres postgres     6 Jul 11 11:47 pg_stat
drwx------ 2 postgres postgres     6 Jul 11 11:47 pg_stat_tmp
drwx------ 2 postgres postgres     6 Jul 11 11:47 pg_subtrans
drwx------ 2 postgres postgres     6 Jul 11 11:47 pg_tblspc
drwx------ 2 postgres postgres     6 Jul 11 11:47 pg_twophase
-rw------- 1 postgres postgres     3 Jul 11 11:47 PG_VERSION
drwx------ 3 postgres postgres    60 Jul 11 11:47 pg_wal
drwx------ 2 postgres postgres    18 Jul 11 11:47 pg_xact
-rw------- 1 postgres postgres   209 Jul 11 11:47 postgresql.auto.conf
-rw------- 1 postgres postgres 26843 Jul 11 11:47 postgresql.conf
-rw------- 1 postgres postgres    20 Jul 11 11:47 standby.signal
```



We start the postgresql service in both the standby nodes and enable the service, run the following command in each standby node as the postgres user to register it with repmgr

```
-bash-4.2$ 
sudo systemctl start postgresql-12
sudo systemctl enable postgresql-12
sudo systemctl status postgresql-12


-bash-4.2$ /usr/pgsql-12/bin/repmgr -f /etc/repmgr/12/repmgr.conf standby register
INFO: connecting to local node "PG-Node2" (ID: 2)
INFO: connecting to primary database
WARNING: --upstream-node-id not supplied, assuming upstream node is primary (node ID 1)
INFO: standby registration complete
NOTICE: standby node "PG-Node2" (ID: 2) successfully registered


-bash-4.2$ /usr/pgsql-12/bin/repmgr -f /etc/repmgr/12/repmgr.conf standby register
INFO: connecting to local node "PG-Node3" (ID: 3)
INFO: connecting to primary database
WARNING: --upstream-node-id not supplied, assuming upstream node is primary (node ID 1)
INFO: standby registration complete
NOTICE: standby node "PG-Node3" (ID: 3) successfully registered
```







Checking from master nodes

```
-bash-4.2$ psql -x -c "select * from pg_stat_replication"
-[ RECORD 1 ]----+------------------------------
pid              | 1131
usesysid         | 16384
usename          | repmgr
application_name | PG-Node3
client_addr      | 10.10.2.103
client_hostname  | 
client_port      | 53900
backend_start    | 2021-07-11 11:54:39.70364+00
backend_xmin     | 
state            | streaming
sent_lsn         | 0/80007D0
write_lsn        | 0/80007D0
flush_lsn        | 0/80007D0
replay_lsn       | 0/80007D0
write_lag        | 
flush_lag        | 
replay_lag       | 
sync_priority    | 0
sync_state       | async
reply_time       | 2021-07-11 11:55:20.786662+00
-[ RECORD 2 ]----+------------------------------
pid              | 1134
usesysid         | 16384
usename          | repmgr
application_name | PG-Node2
client_addr      | 10.10.2.102
client_hostname  | 
client_port      | 44198
backend_start    | 2021-07-11 11:54:52.745014+00
backend_xmin     | 
state            | streaming
sent_lsn         | 0/80007D0
write_lsn        | 0/80007D0
flush_lsn        | 0/80007D0
replay_lsn       | 0/80007D0
write_lag        | 
flush_lag        | 
replay_lag       | 
sync_priority    | 0
sync_state       | async
reply_time       | 2021-07-11 11:55:20.779531+00


-bash-4.2$ psql -c "\x auto  " -c "select pg_is_in_recovery();"
 pg_is_in_recovery 
-------------------
 f
(1 row)

-bash-4.2$ /usr/pgsql-12/bin/repmgr -f /etc/repmgr/12/repmgr.conf cluster show --compact
 ID | Name     | Role    | Status    | Upstream | Location | Prio. | TLI
----+----------+---------+-----------+----------+----------+-------+-----
 1  | PG-Node1 | primary | * running |          | default  | 100   | 1   
 2  | PG-Node2 | standby |   running | PG-Node1 | default  | 100   | 1   
 3  | PG-Node3 | standby |   running | PG-Node1 | default  | 100   | 1   
 
 
```





create data in master

```
rxu_test_db=> CREATE TABLE random_data (c1 bigint  primary key,c2 bigint,c3 time,c4 character varying(136));
CREATE TABLE
rxu_test_db=> insert into random_data(c1,c2,c3,c4) values (random()*10000000, random()*1000,now(),md5(random()::text));
INSERT 0 1
rxu_test_db=> select * from random_data;
   c1    | c2  |       c3        |                c4                
---------+-----+-----------------+----------------------------------
 6479453 | 739 | 10:36:43.982998 | cf66f9b9050402a2b52aa6b4c3b32b5d
(1 row)
```



checking slave data

```
rxu_test_db=> select * from random_data;
   c1    | c2  |       c3        |                c4                
---------+-----+-----------------+----------------------------------
 6479453 | 739 | 10:36:43.982998 | cf66f9b9050402a2b52aa6b4c3b32b5d
(1 row)
```



check slave status

```
-bash-4.2$ psql -c "\x auto  " -c "SELECT * FROM pg_stat_wal_receiver;"
Expanded display is used automatically.
-[ RECORD 1 ]---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
pid                   | 25598
status                | streaming
receive_start_lsn     | 0/7000000
receive_start_tli     | 1
received_lsn          | 0/70001C0
received_tli          | 1
last_msg_send_time    | 2021-07-08 04:08:26.699612+00
last_msg_receipt_time | 2021-07-08 04:08:26.699922+00
latest_end_lsn        | 0/70001C0
latest_end_time       | 2021-07-08 04:07:56.60742+00
slot_name             | 
sender_host           | MASTER_IP
sender_port           | 5432
conninfo              | user=replica password=******** dbname=replication host=MASTER_IP port=5432 fallback_application_name=walreceiver sslmode=prefer sslcompression=0 gssencmode=prefer krbsrvname=postgres target_session_attrs=any

-bash-4.2$ psql -c "\x auto  " -c "select pg_is_in_recovery();"
Expanded display is used automatically.
 pg_is_in_recovery 
-------------------
 t
(1 row)
```







## enable archiving  for recovery

Most of the time, the default or modified retention settings of WAL segments on the Master may not be enough to maintain a healthy replication between itself and its standby. So, we need the WALs to be safely archived to another disk or a remote backup server. These archived WAL segments can be used by the standby to replay them when the WALs are gone from the Master.

To enable archiving on the Master, we can still use the same approach of setting the following 2 parameters.

```
archive_mode = ON
archive_command = 'test ! -f /pg_archive/%f && cp %p /pg_archive/%f'  ## Modify this with an appropriate shell command.
```



starting from PostgreSQL 12, we can add the same parameter to postgresql.conf or postgresql.auto.conf file of the standby. Please note that it requires a restart of PostgreSQL to update the changes made to archive_mode and restore_command parameters.

```
echo "restore_command = 'cp /pg_archive/%f %p'" >> $PGDATA/postgresql.auto.conf

systemctl restart postgresql-12
```





# 主从高可用 (repmgr)

Whenever a primary becomes unavailable, a standby does *not* automatically promote itself to the primary role. A standby still continues to serve read-only queries – although the data will be current up to the last LSN received from the primary. Any attempt for a write operation will fail.

There are two ways to mitigate this:

- The standby is *manually* upgraded to a primary role. This is usually the case for a planned failover or “switchover”
- The standby is *automatically* promoted to a primary role. This is the case with non-native tools that continuously monitor replication and take recovery action when the primary is unavailable. repmgr is one such tool.



## witness node

repmgr uses something called a **witness node**. When the primary is unavailable – it is the witness node’s job to help the standbys reach a quorum if one of them should be promoted to a primary role. The standbys reach this quorum by determining if the primary node is actually offline or only temporarily unavailable. The witness node should be located in the same data centre/network segment/subnet as the primary node, but must NEVER run on the same physical host as the primary node. 



Remember that in the first part of this series, we rolled out a witness node in the same availability zone and subnet as the primary node. We named it PG-Node-Witness and installed a PostgreSQL 12 instance there. In this post, we will install repmgr there as well, but more on that later.

The second component of the solution is the **repmgr daemon (repmgrd)** running in all nodes of the cluster and the witness node. Again, we did not start this daemon in the first part of this series, but we will do so here. The daemon comes as part of the repmgr package – when enabled, it runs as a regular service and continuously monitors the cluster’s health. It initiates a failover when a quorum is reached about the primary being offline. *Not only can it automatically promote a standby, it can also reinitiate other standbys in a multi-node cluster to follow the new primary*.



## The Quorum Process

When a standby realizes it cannot see the primary, it consults with other standbys. All the standbys running in the cluster reach a quorum to choose a new primary using a series of checks:

- Each standby interrogates other standbys about the time it last “saw” the primary. If a standby’s last replicated LSN or the time of last communication with the primary is more recent than the current node’s last replicated LSN or the time of last communication, the node does not do anything and waits for the communication with the primary to be restored
- If none of the standbys can see the primary, they check if the witness node is available. If the witness node cannot be reached either, the standbys assume there is a network outage on the primary side and do not proceed to choose a new primary
- If the witness can be reached, the standbys assume the primary is down and proceed to choose a primary
- The node that was configured as the “preferred” primary will then be promoted. Each standby will have its replication reinitialized to follow the new primary.





## configure witness node

```
-bash-4.2$ sudo yum -y install repmgr12


cat >> 12/data/postgresql.conf <<EOF
listen_addresses = '*'
shared_preload_libraries = 'repmgr'
EOF


cat >> 12/data/pg_hba.conf <<EOF
local   replication     repmgr                     trust
host    replication     repmgr 127.0.0.1/32        trust
host    replication     repmgr 10.10.2.0/24         trust

local   repmgr         repmgr                     trust
host    repmgr         repmgr 127.0.0.1/32        trust
host    repmgr         repmgr 10.10.2.0/24         trust
EOF




createuser --superuser repmgr
createdb --owner=repmgr repmgr
psql -c "ALTER USER repmgr SET search_path TO repmgr, public;"

```



```
sudo vim /etc/repmgr/12/repmgr.conf
node_id=4
node_name='PG-Node-Witness'
conninfo='host=10.10.2.104 user=repmgr dbname=repmgr connect_timeout=2'
data_directory='/var/lib/pgsql/12/data'



sudo systemctl restart postgresql-12
sudo systemctl status postgresql-12
```



## testing result

To test the connectivity to witness node repmgr, we can run this command from the primary node

```
-bash-4.2$ psql 'host=10.10.2.101 user=repmgr dbname=repmgr connect_timeout=2'
```



we register the witness node with repmgr by running the “repmgr witness register” command as the postgres user. Note how we are using the address of the *primary* node

```
-bash-4.2$ /usr/pgsql-12/bin/repmgr -h 10.10.2.101  -U repmgr -d repmgr  -f /etc/repmgr/12/repmgr.conf witness register
INFO: connecting to witness node "PG-Node-Witness" (ID: 4)
INFO: connecting to primary node
NOTICE: attempting to install extension "repmgr"
NOTICE: "repmgr" extension successfully installed
INFO: witness registration complete
NOTICE: witness node "PG-Node-Witness" (ID: 4) successfully registered
```

check the status at any nodes

```
-bash-4.2$ /usr/pgsql-12/bin/repmgr -f /etc/repmgr/12/repmgr.conf cluster show --compact
 ID | Name            | Role    | Status    | Upstream | Location | Prio. | TLI
----+-----------------+---------+-----------+----------+----------+-------+-----
 1  | PG-Node1        | primary | * running |          | default  | 100   | 1   
 2  | PG-Node2        | standby |   running | PG-Node1 | default  | 100   | 1   
 3  | PG-Node3        | standby |   running | PG-Node1 | default  | 100   | 1   
 4  | PG-Node-Witness | witness | * running | PG-Node1 | default  | 0     | n/a 
```





# 同步流程

postgresql主从流复制的流程

![image-20210707224738582](/Users/rxu/coding/github/repo_xuxuehua.com/content/Postgresql/replication.assets/image-20210707224738582.png)



1. 主备数据库启动，备库启动walreceiver进程，wal进程向主库发送连接请求
2. 主库收到连接请求后启动walsender进程，并与walreceiver进程建立tcp连接
3. 备库walreceiver进程发送最新的wal lsn给主库
4. 主库进行lsn对比，定期向备库发送心跳信息来确认备库可用性，并且将没有传递的wal日志进行发送，同时调用SyncRepWaitForLSN()函数来获取锁存器，并且等待备库响应，锁存器的释放时机和主备同步模式的选择有关，同步模式这块内容下面再说
5. 备库调用操作系统write()函数将wal写入缓存，然后调用操作系统fsync()函数将wal刷新到磁盘，然后进行wal回放。同时备库向主库返回ack信息，ack信息中包含write_lsn、flush_lsn、replay_lsn，这些信息会发送给主库，用以告知主库当前wal日志在备库的应用位置及状态，相关位置信息可以通过pg_stat_replication视图查看
6. 如果启用了hot_standby_feedback参数，备库会定期向主库发送xmin信息，用以保证主库不会vacuum掉备库需要的元组信息













# pgpool load balancing



First, we create the directory `archivedir` to store WAL segments on all servers. In this example, only Primary node archives WAL locally.

```
-bash-4.2$ mkdir $PGDATA/pg_archive
-bash-4.2$ ls -l 12/
total 8
drwx------  2 postgres postgres    6 May 13 14:02 backups
drwx------ 21 postgres postgres 4096 Jul  8 10:40 data
-rw-------  1 postgres postgres  911 Jul  5 08:52 initdb.log
drwx------  2 postgres postgres  294 Jul  8 04:02 pg_archive
```



Then we edit the configuration file `$PGDATA/postgresql.conf` on `server1` (primary) as follows. Enable `wal_log_hints` to use `pg_rewind`. Since the Primary may become a Standby later, we set `hot_standby = on`.

```
listen_addresses = '*'
archive_mode = on
archive_command = 'test ! -f /pg_archive/%f && cp %p /pg_archive/%f'
max_wal_senders = 10
max_replication_slots = 10
wal_level = replica
hot_standby = on
wal_log_hints = on
```

We use the online recovery functionality of Pgpool-II to setup standby server after the primary server is started.







## create users

```
# PostgreSQL replication user
postgres=# CREATE ROLE pgpool WITH LOGIN ENCRYPTED PASSWORD 'pgpool';


# Pgpool-II health check (health_check_user) and replication delay check (sr_check_user) user
postgres=# CREATE ROLE repl WITH REPLICATION LOGIN ENCRYPTED PASSWORD 'repl';
CREATE ROLE
```



If you want to show "replication_state" and "replication_sync_state" column in [SHOW POOL NODES](https://www.pgpool.net/docs/pgpool-II-4.2.3/en/html/sql-show-pool-nodes.html) command result, role `pgpool` needs to be PostgreSQL super user or or in `pg_monitor` group (Pgpool-II 4.1 or later). Grant `pg_monitor` to `pgpool`:

```
GRANT pg_monitor TO pgpool;
```





Assuming that all the Pgpool-II servers and the PostgreSQL servers are in the same subnet and edit `pg_hba.conf`

```
host    all             all             SAME_SUBNET                md5
```



## Automated failover

To use the automated failover and online recovery of Pgpool-II, the settings that allow *passwordless* SSH to all backend servers between Pgpool-II execution user (default root user) and `postgres` user and between `postgres` user and `postgres` user are necessary. Execute the following command on all servers to set up passwordless `SSH`. The generated key file name is `id_rsa_pgpool`.

```
[all servers]# 
cd ~/.ssh
ssh-keygen -t rsa -f id_rsa_pgpool
ssh-copy-id -i id_rsa_pgpool.pub postgres@server1
ssh-copy-id -i id_rsa_pgpool.pub postgres@server2
ssh-copy-id -i id_rsa_pgpool.pub postgres@server3

[all servers]# 
su - postgres
cd ~/.ssh
ssh-keygen -t rsa -f id_rsa_pgpool
ssh-copy-id -i id_rsa_pgpool.pub postgres@server1
ssh-copy-id -i id_rsa_pgpool.pub postgres@server2
ssh-copy-id -i id_rsa_pgpool.pub postgres@server3
```



To allow `repl` user without specifying password for streaming replication and online recovery, and execute pg_rewind using `postgres`, we create the `.pgpass` file in `postgres` user's home directory and change the permission to `600` on each PostgreSQL server.

```
[all servers]# 
su - postgres
vi /var/lib/pgsql/.pgpass
server1:5432:replication:repl:<repl user password>
server2:5432:replication:repl:<repl user passowrd>
server3:5432:replication:repl:<repl user passowrd>
server1:5432:postgres:postgres:<postgres user passowrd>
server2:5432:postgres:postgres:<postgres user passowrd>
server3:5432:postgres:postgres:<postgres user passowrd>

[all servers]$ chmod 600  /var/lib/pgsql/.pgpass
```





## pgpool_node_id

From Pgpool-II 4.2, now all configuration parameters are identical on all hosts. If `watchdog` feature is enabled, to distingusish which host is which, a `pgpool_node_id` file is required. You need to create a `pgpool_node_id` file and specify the pgpool (watchdog) node number (e.g. 0, 1, 2 ...) to identify pgpool (watchdog) host.



`server1`

```
[server1]# cat /etc/pgpool-II/pgpool_node_id
0
```



`server2`

```
[server2]# cat /etc/pgpool-II/pgpool_node_id
1
```



`server3`

```
[server3]# cat /etc/pgpool-II/pgpool_node_id
2
```





## Clustering mode

Pgpool-II has several clustering modes. To set the clustering mode, [backend_clustering_mode](https://www.pgpool.net/docs/pgpool-II-4.2.3/en/html/runtime-config-running-mode.html#GUC-BACKEND-CLUSTERING-MODE) can be used. In this configuration example, streaming replication mode is used.

When installing Pgpool-II using RPM, all the Pgpool-II configuration sample files are in `/etc/pgpool-II`. In this example, we copy the sample configuration file for streaming replication mode.

```
cp -p /etc/pgpool-II/pgpool.conf.sample-stream /etc/pgpool-II/pgpool.conf
```



```
vim /etc/pgpool-II/pgpool.conf
listen_addresses = '*'
port = 9999


sr_check_user = 'pgpool'
sr_check_password = ''


health_check_period = 5
                                            # Health check period
                                            # Disabled (0) by default
health_check_timeout = 30
                                            # Health check timeout
                                            # 0 means no timeout
health_check_user = 'pgpool'
health_check_password = ''

health_check_max_retries = 3



# - Backend Connection Settings -

backend_hostname0 = 'server1'
                                            # Host name or IP address to connect to for backend 0
backend_port0 = 5432
                                            # Port number for backend 0
backend_weight0 = 1
                                            # Weight for backend 0 (only in load balancing mode)
backend_data_directory0 = '/var/lib/pgsql/12/data'
                                            # Data directory for backend 0
backend_flag0 = 'ALLOW_TO_FAILOVER'
                                            # Controls various backend behavior
                                            # ALLOW_TO_FAILOVER or DISALLOW_TO_FAILOVER
backend_hostname1 = 'server2'
backend_port1 = 5432
backend_weight1 = 1
backend_data_directory1 = '/var/lib/pgsql/13/data'
backend_flag1 = 'ALLOW_TO_FAILOVER'

backend_hostname2 = 'server3'
backend_port2 = 5432
backend_weight2 = 1
backend_data_directory2 = '/var/lib/pgsql/13/data'
backend_flag2 = 'ALLOW_TO_FAILOVER'
```



# Appendix

数据库架构之美

https://www.percona.com/blog/2019/10/11/how-to-set-up-streaming-replication-in-postgresql-12/

https://www.pgpool.net/docs/pgpool-II-4.2.3/en/html/example-configs.html

https://www.2ndquadrant.com/en/blog/how-to-automate-postgresql-12-replication-and-failover-with-repmgr-part-1/

