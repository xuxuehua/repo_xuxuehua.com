---
title: "incident"
date: 2020-10-21 08:55
---
[toc]







# 故障前的准备工作

**以用户功能为索引的服务和资源的全视图**。首先，我们需要一个系统来记录前端用户操作界面和后端服务，以及服务所使用到的硬件资源之间的关联关系。这个系统有点像 CMDB（配置管理数据库），但是比 CMDB 要大得多，是以用户端的功能来做索引的。然后，把后端的服务、服务的调用关系，以及服务使用到的资源都关联起来做成一个视图。如开源的https://github.com/openzipkin/zipkin



**为地图中的各个服务制订关键指标，以及一套运维流程和工具，包括应急方案**。以用户功能为索引，为每个用户功能的服务都制订一个服务故障的检测、处理和恢复手册，以及相关的检测、查错或是恢复的运维工具。对于基础层和一些通用的中间件，也需要有相应的最佳实践的方法。



**设定故障的等级**。还要设定不同故障等级的处理方式。比如，亚马逊一般将故障分为 4 级：1 级是全站不可用；2 级是某功能不可用，且无替代方案；3 级是某功能不可用，但有替代方案；4 级是非功能性故障，或是用户不关心的故障。阿里内的分类更多样一些，有时会根据影响多少用户来定故障等级。



**故障演练**。故障是需要演练的。因为故障并不会时常发生，但我们又需要不断提升处理故障的能力，所以需要经常演练。一些大公司，如 Netflix，会有一个叫 Chaos Monkey 的东西，随机地在生产线上乱来。Facebook 也会有一些故障演习，比如，随机关掉线上的一些服务器。总之，要提升故障处理水平，最好的方式就是实践。见得多了，处理得多了，才能驾轻就熟。**故障演练是一个非常好的实践**。



**灰度发布系统**。要减少线上故障的影响范围，通过灰度发布系统来发布是一个很不错的方式。毕竟，我们在测试环境中很难模拟出线上环境的所有情况，所以，在生产线上进行灰度发布或是 A/B 测试是一件很好的事。





# 故障发生时

故障源团队通常会有以下几种手段来恢复系统。

- **重启和限流**。重启和限流主要解决的是可用性的问题，不是功能性的问题。重启还好说，但是限流这个事就需要相关的流控中间件了。
- **回滚操作**。回滚操作一般来说是解决新代码的 bug，把代码回滚到之前的版本是快速的方式。
- **降级操作**。并不是所有的代码变更都是能够被回滚的，如果无法回滚，就需要降级功能了。也就是说，需要挂一个停止服务的故障公告，主要是不要把事态扩大。
- **紧急更新**。紧急更新是常用的手段，这个需要强大的自动化系统，尤其是自动化测试和自动化发布系统。假如你要紧急更新 1000 多台服务器，没有一个强大的自动化发布系统是很难做到的。

也就是说，出现故障时，**最重要的不是 debug 故障，而是尽可能地减少故障的影响范围，并尽可能快地修复问题**。



# 故障处理

**一个技术问题，后面隐藏的是工程能力问题，工程能力问题后面隐藏的是管理问题，管理问题后面隐藏的是一个公司文化的问题，公司文化的问题则隐藏着创始人的问题……**



## 原则

1. **举一反三解决当下的故障**。为自己赢得更多的时间。
2. **简化复杂、不合理的技术架构、流程和组织**。你不可能在一个复杂的环境下根本地解决问题。
3. **全面改善和优化整个系统，包括组织**。解决问题的根本方法是改善和调整整体结构。而只有简单优雅的东西才有被改善和优化的可能。



换句话说，我看到很多问题出了又出，换着花样地出，大多数情况下是因为这个公司的系统架构太过复杂和混乱，以至于你不可能在这样的环境下干干净净地解决所有的问题。所以，你要先做大扫除，简化掉现有的复杂和混乱。如果你要从根本上改善一个事，那么首先得把它简化了。这就是这么多年来，我得到的认识。

但是，很不幸，我们就是生活在这样一个复杂的世界，有太多的人喜欢把简单的问题复杂化。所以，要想做到简化，基本上来说是非常非常难的。





# 故障复盘

可以把所有的相关人员都叫到现场进行复盘，复盘会会开很长时间，但是把大家叫在一起复盘的确是一个很好的方式。一方面信息是透明的，另一方面，也是对大家的一次教育。

**故障处理的整个过程**。就像一个 log 一样，需要详细地记录几点几分干了什么事，把故障从发生到解决的所有细节过程都记录下来。

**故障原因分析**。需要说明故障的原因和分析报告。

**Ask 5 Whys**。需要反思并反问至少 5 个为什么，并为这些“为什么”找到答案。

**故障后续整改计划**。需要针对上述的“Ask 5 Whys”说明后续如何举一反三地从根本上解决所有的问题。





## example

一个慢 SQL 的故障复盘时，我一共问了近 9 个为什么。

1. 为什么从故障发生到系统报警花了 27 分钟？为什么只发邮件，没有短信？
2. 为什么花了 15 分钟，开发的同学才知道是慢 SQL 问题？
3. 为什么监控系统没有监测到 Nginx 499 错误，以及 Nginx 的 upstream_response_time 和 request_time？
4. 为什么在一开始按 DDoS 处理？
5. 为什么要重启数据库？
6. 为什么这个故障之前没有发生？因为以前没有上首页，最近上的。
7. 为什么上首页时没有做性能测试？
8. 为什么使用这个高危的 SQL 语句？
9. 上线过程中为什么没有 DBA 评审？

通过这 9 个为什么，我为这家公司整理出来很多不足的地方。提出这些问题的大致逻辑是这样的。

第一，优化故障获知和故障定位的时间。

- 从故障发生到我们知道的时间是否可以优化得更短？
- 定位故障的时间是否可以更短？
- 有哪些地方可以做到自动化？

第二，优化故障的处理方式。

- 故障处理时的判断和章法是否科学，是否正确？
- 故障处理时的信息是否全透明？
- 故障处理时人员是否安排得当？

第三，优化开发过程中的问题。

- Code Review 和测试中的问题和优化点。
- 软件架构和设计是否可以更好？
- 对于技术欠债或是相关的隐患问题是否被记录下来，是否有风险计划？

第四，优化团队能力。

- 如何提高团队的技术能力？
- 如何让团队有严谨的工程意识？

具体采取什么样的整改方案会和这些为什么很有关系。

总之还是那句话，解决一个故障可以通过技术和管理两方面的方法。如果你喜欢技术，是个技术范，你就更多地用技术手段；如果你喜欢管理，那么你就会使用更多的管理手段。



