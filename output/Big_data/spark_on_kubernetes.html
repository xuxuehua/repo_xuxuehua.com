<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/static/css/tango.css">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>spark_on_kubernetes - Xu XueHua</title>
    <meta name="keywords" content="Xu XueHua"/>
    <meta name="description" content="Xu XueHua's public notes"/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
    <div id="container">
      
<div id="header">
  <div class="post-nav"><a href="/">Home</a>&nbsp;&#187;&nbsp;<a href="/#Big_data">Big_data</a>&nbsp;&#187;&nbsp;spark_on_kubernetes
    <span class="updated">Page Updated&nbsp;
      2019-10-11 13:45
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">spark_on_kubernetes</div>

  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#spark-on-k8s">Spark on K8s</a><ul>
<li><a href="#_1">特点</a></li>
<li><a href="#_2">基本原理</a><ul>
<li><a href="#_3">用户及资源隔离</a><ul>
<li><a href="#kubernetes-namespace">Kubernetes Namespace</a></li>
<li><a href="#namespace-node">namespace -&gt; node</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#requirements">requirements</a></li>
</ul>
</li>
<li><a href="#installation">installation</a><ul>
<li><a href="#_4">镜像</a></li>
<li><a href="#_5">网络</a></li>
<li><a href="#kube-dns">Kube-dns</a></li>
<li><a href="#kubernetes-ha">Kubernetes HA</a></li>
<li><a href="#_6">日志</a></li>
</ul>
</li>
<li><a href="#_7">操作</a><ul>
<li><a href="#docker">Docker镜像</a></li>
<li><a href="#_8">提交作业</a></li>
<li><a href="#spark-driver-ui">Spark Driver UI</a></li>
<li><a href="#_9">访问日志</a></li>
</ul>
</li>
</ul>
</div>
<h1 id="spark-on-k8s">Spark on K8s</h1>
<p>利用原生的Kubernetes调度器进行集群资源的分配与管理Spark应用。在此模式下，Spark Driver和Executor均使用Pod来运行，进一步可通过指定Node Selector功能将应用运行于特定节点之上（如：带有GPU的节点实例）</p>
<h2 id="_1">特点</h2>
<p>使用 kubernetes 原生调度的 spark on kubernetes 是对原有的 spark on yarn 革命性的改变，主要表现在以下几点：</p>
<ol>
<li>Kubernetes 原生调度：不再需要二层调度，直接使用 kubernetes 的资源调度功能，跟其他应用共用整个 kubernetes 管理的资源池；</li>
<li>资源隔离，粒度更细：原先 yarn 中的 queue 在 spark on kubernetes 中已不存在，取而代之的是 kubernetes 中原生的 namespace，可以为每个用户分别指定一个 namespace，限制用户的资源 quota；</li>
<li>细粒度的资源分配：可以给每个 spark 任务指定资源限制，实际指定多少资源就使用多少资源，因为没有了像 yarn 那样的二层调度（圈地式的），所以可以更高效和细粒度的使用资源；</li>
<li>监控的变革：因为做到了细粒度的资源分配，所以可以对用户提交的每一个任务做到资源使用的监控，从而判断用户的资源使用情况，所有的 metric 都记录在数据库中，甚至可以为每个用户的每次任务提交计量；</li>
<li>日志的变革：用户不再通过 yarn 的 web 页面来查看任务状态，而是通过 pod 的 log 来查看，可将所有的 kuberentes 中的应用的日志等同看待收集起来，然后可以根据标签查看对应应用的日志；</li>
</ol>
<p>所有这些变革都能帮助我们更高效的获的、有效的利用资源，提高生产效率。</p>
<h2 id="_2">基本原理</h2>
<p><img alt="image-20200405155837123" src="spark_on_kubernetes.assets/image-20200405155837123.png" /></p>
<p><code>spark-submit</code>将Spark作业提交到Kubernetes集群时，会执行以下流程：</p>
<ul>
<li>Spark在Kubernetes pod中创建Spark driver</li>
<li>Driver调用Kubernetes API创建executor pods，executor pods执行作业代码</li>
<li>计算作业结束，executor pods回收并清理</li>
<li>driver pod处于<code>completed</code>状态，保留日志，直到Kubernetes GC或者手动清理</li>
</ul>
<h3 id="_3">用户及资源隔离</h3>
<h4 id="kubernetes-namespace">Kubernetes Namespace</h4>
<p>在Kubernetes中，我们可以使用namespace在多用户间实现资源分配、隔离和配额。Spark On Kubernetes同样支持配置namespace创建Spark作业。</p>
<p>首先，创建一个Kubernetes namespace：</p>
<div class="hlcode"><pre><span class="err">$</span> <span class="n">kubectl</span> <span class="n">create</span> <span class="n">namespace</span> <span class="n">spark</span>
</pre></div>


<p>由于我们的Kubernetes集群使用了RBAC，所以还需创建serviceaccount和绑定role：</p>
<div class="hlcode"><pre><span class="err">$</span> <span class="n">kubectl</span> <span class="n">create</span> <span class="n">serviceaccount</span> <span class="n">spark</span> <span class="o">-</span><span class="n">n</span> <span class="n">spark</span>
<span class="err">$</span> <span class="n">kubectl</span> <span class="n">create</span> <span class="n">clusterrolebinding</span> <span class="n">spark</span><span class="o">-</span><span class="n">role</span> <span class="o">--</span><span class="n">clusterrole</span><span class="o">=</span><span class="n">edit</span> <span class="o">--</span><span class="n">serviceaccount</span><span class="o">=</span><span class="n">spark</span><span class="o">:</span><span class="n">spark</span> <span class="o">--</span><span class="n">namespace</span><span class="o">=</span><span class="n">spark</span>
</pre></div>


<p>并在spark-submit中新增以下配置：</p>
<div class="hlcode"><pre><span class="err">$</span> <span class="n">bin</span><span class="o">/</span><span class="n">spark</span><span class="o">-</span><span class="n">submit</span> \
    <span class="o">--</span><span class="n">conf</span> <span class="n">spark</span><span class="p">.</span><span class="n">kubernetes</span><span class="p">.</span><span class="n">authenticate</span><span class="p">.</span><span class="n">driver</span><span class="p">.</span><span class="n">serviceAccountName</span><span class="o">=</span><span class="n">spark</span> \
    <span class="o">--</span><span class="n">conf</span> <span class="n">spark</span><span class="p">.</span><span class="n">kubernetes</span><span class="p">.</span><span class="n">namespace</span><span class="o">=</span><span class="n">spark</span> \
    <span class="p">...</span>
</pre></div>


<h4 id="namespace-node">namespace -&gt; node</h4>
<p>考虑到我们Spark作业的一些特点和计算资源隔离，前期我们还是选择了较稳妥的物理隔离方案。具体做法是为每个组提供单独的Kubernetes namespace，计算任务都在各自namespace里提交。计算资源以物理机为单位，折算成cpu和内存，纳入Kubernetes统一管理。在Kubernetes集群里，通过<code>node label</code>和<code>PodNodeSelector</code>将计算资源和namespace关联。从而实现在提交Spark作业时，计算资源总是选择namespace关联的node。</p>
<p>具体做法如下：</p>
<p>1、创建node label</p>
<div class="hlcode"><pre><span class="err">$</span> <span class="nx">kubectl</span> <span class="nb">label</span> <span class="nx">nodes</span> <span class="o">&lt;</span><span class="nx">node_name</span><span class="o">&gt;</span> <span class="nx">spark</span><span class="p">:</span><span class="nx">spark</span>
</pre></div>


<p>2、开启Kubernetes admission controller<br />
我们是使用kubeadm安装Kubernetes集群，所以修改/etc/kubernetes/manifests/kube-apiserver.yaml，在<code>--admission-control</code>后添加<code>PodNodeSelector</code></p>
<div class="hlcode"><pre><span class="err">$</span> <span class="n">cat</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">kubernetes</span><span class="o">/</span><span class="n">manifests</span><span class="o">/</span><span class="n">kube</span><span class="o">-</span><span class="n">apiserver</span><span class="p">.</span><span class="n">yaml</span>
<span class="nl">apiVersion:</span> <span class="n">v1</span>
<span class="nl">kind:</span> <span class="n">Pod</span>
<span class="nl">metadata:</span>
  <span class="nl">annotations:</span>
    <span class="n">scheduler</span><span class="p">.</span><span class="n">alpha</span><span class="p">.</span><span class="n">kubernetes</span><span class="p">.</span><span class="n">io</span><span class="o">/</span><span class="n">critical</span><span class="o">-</span><span class="n">pod</span><span class="o">:</span> <span class="s">&quot;&quot;</span>
  <span class="nl">creationTimestamp:</span> <span class="n">null</span>
  <span class="nl">labels:</span>
    <span class="nl">component:</span> <span class="n">kube</span><span class="o">-</span><span class="n">apiserver</span>
    <span class="nl">tier:</span> <span class="n">control</span><span class="o">-</span><span class="n">plane</span>
  <span class="nl">name:</span> <span class="n">kube</span><span class="o">-</span><span class="n">apiserver</span>
  <span class="nl">namespace:</span> <span class="n">kube</span><span class="o">-</span><span class="n">system</span>
<span class="nl">spec:</span>
  <span class="nl">containers:</span>
  <span class="o">-</span> <span class="n">command</span><span class="o">:</span>
    <span class="o">-</span> <span class="n">kube</span><span class="o">-</span><span class="n">apiserver</span>
    <span class="o">-</span> <span class="o">--</span><span class="n">secure</span><span class="o">-</span><span class="n">port</span><span class="o">=</span><span class="mi">6443</span>
    <span class="o">-</span> <span class="o">--</span><span class="n">proxy</span><span class="o">-</span><span class="n">client</span><span class="o">-</span><span class="n">cert</span><span class="o">-</span><span class="n">file</span><span class="o">=/</span><span class="n">etc</span><span class="o">/</span><span class="n">kubernetes</span><span class="o">/</span><span class="n">pki</span><span class="o">/</span><span class="n">front</span><span class="o">-</span><span class="n">proxy</span><span class="o">-</span><span class="n">client</span><span class="p">.</span><span class="n">crt</span>
    <span class="o">-</span> <span class="o">--</span><span class="n">admission</span><span class="o">-</span><span class="n">control</span><span class="o">=</span><span class="n">Initializers</span><span class="p">,</span><span class="n">NamespaceLifecycle</span><span class="p">,</span><span class="n">LimitRanger</span><span class="p">,</span><span class="n">ServiceAccount</span><span class="p">,</span><span class="n">DefaultStorageClass</span><span class="p">,</span><span class="n">DefaultTolerationSeconds</span><span class="p">,</span><span class="n">NodeRestriction</span><span class="p">,</span><span class="n">ResourceQuota</span><span class="p">,</span><span class="n">MutatingAdmissionWebhook</span><span class="p">,</span><span class="n">ValidatingAdmissionWebhook</span><span class="p">,</span><span class="n">PodNodeSelector</span>
<span class="p">...</span>
</pre></div>


<p>3、配置PodNodeSelector 在namespace的annotations中添加<code>scheduler.alpha.kubernetes.io/node-selector: spark=spark</code>。</p>
<div class="hlcode"><pre><span class="n">apiVersion</span><span class="o">:</span> <span class="n">v1</span>
<span class="n">kind</span><span class="o">:</span> <span class="n">Namespace</span>
<span class="n">metadata</span><span class="o">:</span>
  <span class="n">annotations</span><span class="o">:</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="na">alpha</span><span class="o">.</span><span class="na">kubernetes</span><span class="o">.</span><span class="na">io</span><span class="o">/</span><span class="n">node</span><span class="o">-</span><span class="n">selector</span><span class="o">:</span> <span class="n">spark</span><span class="o">=</span><span class="n">spark</span>
  <span class="n">name</span><span class="o">:</span> <span class="n">spark</span>
</pre></div>


<p>完成以上配置后，可以通过<code>spark-submit</code>测试结果：</p>
<div class="hlcode"><pre><span class="err">$</span> <span class="n">spark</span><span class="o">-</span><span class="n">submit</span>
<span class="o">--</span><span class="n">conf</span> <span class="n">spark</span><span class="p">.</span><span class="n">kubernetes</span><span class="p">.</span><span class="n">authenticate</span><span class="p">.</span><span class="n">driver</span><span class="p">.</span><span class="n">serviceAccountName</span><span class="o">=</span><span class="n">spark</span> 
<span class="o">--</span><span class="n">conf</span> <span class="n">spark</span><span class="p">.</span><span class="n">kubernetes</span><span class="p">.</span><span class="n">namespace</span><span class="o">=</span><span class="n">spark</span> 
<span class="o">--</span><span class="n">master</span> <span class="n">k8s</span><span class="o">:</span><span class="c1">//https://xxxx:6443     </span>
<span class="o">--</span><span class="n">deploy</span><span class="o">-</span><span class="n">mode</span> <span class="n">cluster</span>     
<span class="o">--</span><span class="n">name</span> <span class="n">spark</span><span class="o">-</span><span class="n">pi</span>     
<span class="o">--</span><span class="n">class</span> <span class="n">org</span><span class="p">.</span><span class="n">apache</span><span class="p">.</span><span class="n">spark</span><span class="p">.</span><span class="n">examples</span><span class="p">.</span><span class="n">SparkPi</span>     
<span class="o">--</span><span class="n">conf</span> <span class="n">spark</span><span class="p">.</span><span class="n">executor</span><span class="p">.</span><span class="n">instances</span><span class="o">=</span><span class="mi">5</span>     
<span class="o">--</span><span class="n">conf</span> <span class="n">spark</span><span class="p">.</span><span class="n">kubernetes</span><span class="p">.</span><span class="n">container</span><span class="p">.</span><span class="n">image</span><span class="o">=</span><span class="n">xxxx</span><span class="o">/</span><span class="n">library</span><span class="o">/</span><span class="n">spark</span><span class="o">:</span><span class="n">v2</span><span class="mf">.3</span>     
<span class="nl">http:</span><span class="c1">//xxxx:81/spark-2.3.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.3.0.jar</span>
</pre></div>


<h2 id="requirements">requirements</h2>
<ul>
<li>Spark 2.3+</li>
<li>Kubernetes 1.6+</li>
<li>具有Kubernetes pods的list, create, edit和delete权限</li>
<li>Kubernetes集群必须正确配置<a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/">Kubernetes DNS</a></li>
</ul>
<h1 id="installation">installation</h1>
<div class="hlcode"><pre><span class="nx">kubeadm</span> <span class="nb">init</span>

<span class="nx">kubeadm</span> <span class="k">join</span> <span class="o">--</span><span class="nb">token</span> <span class="o">&lt;</span><span class="nb">token</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="nx">master</span><span class="na">-ip</span><span class="o">&gt;</span><span class="p">:</span><span class="o">&lt;</span><span class="nx">master</span><span class="na">-port</span><span class="o">&gt;</span> <span class="o">--</span><span class="nx">discovery</span><span class="na">-token-ca-cert-hash</span> <span class="nx">sha256</span><span class="p">:</span><span class="o">&lt;</span><span class="nb">hash</span><span class="o">&gt;</span>
</pre></div>


<h2 id="_4">镜像</h2>
<p>很多镜像无法从k8s.gcr.io获取，我们需要将之替换为第三方提供的镜像，比如：https://hub.docker.com/u/mirrorgooglecontainers/。</p>
<h2 id="_5">网络</h2>
<p>Kubernetes网络默认是通过CNI实现，主流的CNI plugin有：Linux Bridge、MACVLAN、Flannel、Calico、Kube-router、Weave Net等。Flannel主要是使用VXLAN tunnel来解决pod间的网络通信，Calico和Kube-router则是使用BGP。由于软VXLAN对宿主机的性能和网络有不小的损耗，BGP则对硬件交换机有一定的要求，且我们的基础网络是VXLAN实现的大二层，所以我们最终选择了MACVLAN。</p>
<p>CNI MACVLAN的配置示例如下：</p>
<div class="hlcode"><pre><span class="p">{</span>
  <span class="s2">&quot;name&quot;</span><span class="o">:</span> <span class="s2">&quot;mynet&quot;</span><span class="o">,</span>
  <span class="s2">&quot;type&quot;</span><span class="o">:</span> <span class="s2">&quot;macvlan&quot;</span><span class="o">,</span>
  <span class="s2">&quot;master&quot;</span><span class="o">:</span> <span class="s2">&quot;eth0&quot;</span><span class="o">,</span>
  <span class="s2">&quot;ipam&quot;</span><span class="o">:</span> <span class="err">{</span>
    <span class="s2">&quot;type&quot;</span><span class="o">:</span> <span class="s2">&quot;host-local&quot;</span><span class="o">,</span>
    <span class="s2">&quot;subnet&quot;</span><span class="o">:</span> <span class="s2">&quot;10.0.0.0/17&quot;</span><span class="o">,</span>
    <span class="s2">&quot;rangeStart&quot;</span><span class="o">:</span> <span class="s2">&quot;10.0.64.1&quot;</span><span class="o">,</span>
    <span class="s2">&quot;rangeEnd&quot;</span><span class="o">:</span> <span class="s2">&quot;10.0.64.126&quot;</span><span class="o">,</span>
    <span class="s2">&quot;gateway&quot;</span><span class="o">:</span> <span class="s2">&quot;10.0.127.254&quot;</span><span class="o">,</span>
    <span class="s2">&quot;routes&quot;</span><span class="o">:</span> <span class="cp">[</span>
      <span class="p">{</span>
        <span class="s2">&quot;dst&quot;</span><span class="p">:</span> <span class="s2">&quot;0.0.0.0/0&quot;</span>
      <span class="p">},</span>
      <span class="p">{</span>
        <span class="s2">&quot;dst&quot;</span><span class="p">:</span> <span class="s2">&quot;10.0.80.0/24&quot;</span><span class="p">,</span>
        <span class="s2">&quot;gw&quot;</span><span class="p">:</span> <span class="s2">&quot;10.0.0.61&quot;</span>
      <span class="p">}</span>
    <span class="cp">]</span>
  <span class="p">}</span>
<span class="err">}</span>
</pre></div>


<p>Pod subnet是10.0.0.0/17，实际pod ip pool是10.0.64.0/20。cluster cidr是10.0.80.0/24。我们使用的IPAM是host-local，规则是在每个Kubernetes node上建立/25的子网，可以提供126个IP。我们还配置了一条到cluster cidr的静态路由<code>10.0.80.0/24</code>，网关是宿主机。这是因为容器在macvlan配置下egress并不会通过宿主机的iptables，这点和Linux Bridge有较大区别。在Linux Bridge模式下，只要指定内核参数<code>net.bridge.bridge-nf-call-iptables = 1</code>，所有进入bridge的流量都会通过宿主机的iptables。经过分析kube-proxy，我们发现可以使用<code>KUBE-FORWARD</code>这个chain来进行pod到service的网络转发：</p>
<div class="hlcode"><pre><span class="o">-</span><span class="n">A</span> <span class="n">FORWARD</span> <span class="o">-</span><span class="n">m</span> <span class="n">comment</span> <span class="o">--</span><span class="n">comment</span> <span class="s">&quot;kubernetes forward rules&quot;</span> <span class="o">-</span><span class="n">j</span> <span class="n">KUBE</span><span class="o">-</span><span class="n">FORWARD</span>
<span class="o">-</span><span class="n">A</span> <span class="n">KUBE</span><span class="o">-</span><span class="n">FORWARD</span> <span class="o">-</span><span class="n">m</span> <span class="n">comment</span> <span class="o">--</span><span class="n">comment</span> <span class="s">&quot;kubernetes forwarding rules&quot;</span> <span class="o">-</span><span class="n">m</span> <span class="n">mark</span> <span class="o">--</span><span class="n">mark</span> <span class="mh">0x4000</span><span class="o">/</span><span class="mh">0x4000</span> <span class="o">-</span><span class="n">j</span> <span class="n">ACCEPT</span>
<span class="o">-</span><span class="n">A</span> <span class="n">KUBE</span><span class="o">-</span><span class="n">FORWARD</span> <span class="o">-</span><span class="n">s</span> <span class="mf">10.0.0.0</span><span class="o">/</span><span class="mi">17</span> <span class="o">-</span><span class="n">m</span> <span class="n">comment</span> <span class="o">--</span><span class="n">comment</span> <span class="s">&quot;kubernetes forwarding conntrack pod source rule&quot;</span> <span class="o">-</span><span class="n">m</span> <span class="n">conntrack</span> <span class="o">--</span><span class="n">ctstate</span> <span class="n">RELATED</span><span class="p">,</span><span class="n">ESTABLISHED</span> <span class="o">-</span><span class="n">j</span> <span class="n">ACCEPT</span>
<span class="o">-</span><span class="n">A</span> <span class="n">KUBE</span><span class="o">-</span><span class="n">FORWARD</span> <span class="o">-</span><span class="n">d</span> <span class="mf">10.0.0.0</span><span class="o">/</span><span class="mi">17</span> <span class="o">-</span><span class="n">m</span> <span class="n">comment</span> <span class="o">--</span><span class="n">comment</span> <span class="s">&quot;kubernetes forwarding conntrack pod destination rule&quot;</span> <span class="o">-</span><span class="n">m</span> <span class="n">conntrack</span> <span class="o">--</span><span class="n">ctstate</span> <span class="n">RELATED</span><span class="p">,</span><span class="n">ESTABLISHED</span> <span class="o">-</span><span class="n">j</span> <span class="n">ACCEPT</span>
</pre></div>


<p>最后通过<code>KUBE-SERVICES</code>使用DNAT到后端的pod。pod访问其他网段的话，就通过物理网关10.0.127.254。</p>
<p>还有一个需要注意的地方是出于kernel security的考虑，link物理接口的macvlan是无法直接和物理接口通信的，这就导致容器并不能将宿主机作为网关。我们采用了一个小技巧，避开了这个限制。我们从物理接口又创建了一个macvlan，将物理IP移到了这个接口上，物理接口只作为网络入口：</p>
<div class="hlcode"><pre><span class="err">$</span> <span class="n">cat</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">sysconfig</span><span class="o">/</span><span class="n">network</span><span class="o">-</span><span class="n">scripts</span><span class="o">/</span><span class="n">ifcfg</span><span class="o">-</span><span class="n">eth0</span>

<span class="n">DEVICE</span><span class="o">=</span><span class="n">eth0</span>
<span class="n">IPV6INIT</span><span class="o">=</span><span class="n">no</span>
<span class="n">BOOTPROTO</span><span class="o">=</span><span class="n">none</span>

<span class="err">$</span> <span class="n">cat</span> <span class="o">/</span><span class="n">etc</span><span class="o">/</span><span class="n">sysconfig</span><span class="o">/</span><span class="n">network</span><span class="o">-</span><span class="n">scripts</span><span class="o">/</span><span class="n">ifcfg</span><span class="o">-</span><span class="n">macvlan</span>

<span class="n">DEVICE</span><span class="o">=</span><span class="n">macvlan</span>
<span class="n">NAME</span><span class="o">=</span><span class="n">macvlan</span>
<span class="n">BOOTPROTO</span><span class="o">=</span><span class="n">none</span>
<span class="n">ONBOOT</span><span class="o">=</span><span class="n">yes</span>
<span class="n">TYPE</span><span class="o">=</span><span class="n">macvlan</span>
<span class="n">DEVICETYPE</span><span class="o">=</span><span class="n">macvlan</span>
<span class="n">DEFROUTE</span><span class="o">=</span><span class="n">yes</span>
<span class="n">PEERDNS</span><span class="o">=</span><span class="n">yes</span>
<span class="n">PEERROUTES</span><span class="o">=</span><span class="n">yes</span>
<span class="n">IPV4_FAILURE_FATAL</span><span class="o">=</span><span class="n">no</span>
<span class="n">IPADDR</span><span class="o">=</span><span class="mf">10.0.0.61</span>
<span class="n">PREFIX</span><span class="o">=</span><span class="mi">17</span>
<span class="n">GATEWAY</span><span class="o">=</span><span class="mf">10.0.127.254</span>
<span class="n">MACVLAN_PARENT</span><span class="o">=</span><span class="n">eth0</span>
<span class="n">MACVLAN_MODE</span><span class="o">=</span><span class="n">bridge</span>
</pre></div>


<blockquote>
<p>这样两个macvlan是可以互相通信的。</p>
</blockquote>
<h2 id="kube-dns">Kube-dns</h2>
<p>默认配置下，Kubernetes使用kube-dns进行DNS解析和服务发现。但在实际使用时，我们发现在pod上通过service domain访问service总是有5秒的延迟。使用tcpdump抓包，发现延迟出现在DNS AAAA。进一步排查，发现问题是由于netfilter在<code>conntrack</code>和<code>SNAT</code>时的<code>Race Condition</code>导致。简言之，DNS A和AAAA记录请求报文是并行发出的，这会导致netfilter在<code>_nf_conntrack_confirm</code>时认为第二个包是重复的（因为有相同的五元组），从而丢包。具体可看我提的issue：https://github.com/kubernetes/kubernetes/issues/62628。一个简单的解决方案是在<code>/etc/resolv.conf</code>中增加<code>options single-request-reopen</code>，使DNS A和AAAA记录请求报文使用不同的源端口。我提的PR在：https://github.com/kubernetes/kubernetes/issues/62628，大家可以参考。我们的解决方法是不使用Kubernetes service，设置<code>hostNetwork=true</code>使用宿主机网络提供DNS服务。因为我们的基础网络是大二层，所以pod和node可以直接通信，这就避免了<code>conntrack</code>和<code>SNAT</code>。</p>
<h2 id="kubernetes-ha">Kubernetes HA</h2>
<p>Kubernetes的集群状态基本都保存在etcd中，所以etcd是HA的关键所在。由于我们目前还处在半生产状态，HA这方面未过多考虑。有兴趣的同学可以查看：https://kubernetes.io/docs/setup/independent/high-availability/</p>
<h2 id="_6">日志</h2>
<p>在Spark On Yarn下，可以开启<code>yarn.log-aggregation-enable</code>将日志收集聚合到HDFS中，以供查看。但是在Spark On Kubernetes中，则缺少这种日志收集机制，我们只能通过Kubernetes pod的日志输出，来查看Spark的日志：</p>
<div class="hlcode"><pre><span class="err">$</span> <span class="nx">kubectl</span> <span class="na">-n</span><span class="o">=&lt;</span><span class="nx">namespace</span><span class="o">&gt;</span> <span class="nx">logs</span> <span class="na">-f</span> <span class="o">&lt;</span><span class="nx">driver</span><span class="na">-pod-name</span><span class="o">&gt;</span>
</pre></div>


<h1 id="_7">操作</h1>
<h2 id="docker">Docker镜像</h2>
<p>由于Spark driver和executor都运行在Kubernetes pod中，并且我们使用Docker作为container runtime enviroment，所以首先我们需要建立Spark的Docker镜像。</p>
<p>在Spark distribution中已包含相应脚本和Dockerfile，可以通过以下命令构建镜像：</p>
<div class="hlcode"><pre><span class="err">$</span> <span class="nx">.</span><span class="p">/</span><span class="nx">bin</span><span class="p">/</span><span class="nx">docker</span><span class="na">-image-tool.sh</span> <span class="na">-r</span> <span class="o">&lt;</span><span class="nx">repo</span><span class="o">&gt;</span> <span class="na">-t</span> <span class="nx">my</span><span class="na">-tag</span> <span class="nx">build</span>
<span class="err">$</span> <span class="nx">.</span><span class="p">/</span><span class="nx">bin</span><span class="p">/</span><span class="nx">docker</span><span class="na">-image-tool.sh</span> <span class="na">-r</span> <span class="o">&lt;</span><span class="nx">repo</span><span class="o">&gt;</span> <span class="na">-t</span> <span class="nx">my</span><span class="na">-tag</span> <span class="nb">push</span>
</pre></div>


<h2 id="_8">提交作业</h2>
<p>在构建Spark镜像后，我们可以通过以下命令提交作业：</p>
<div class="hlcode"><pre><span class="err">$</span> <span class="n">bin</span><span class="o">/</span><span class="n">spark</span><span class="o">-</span><span class="n">submit</span> \
    <span class="o">--</span><span class="n">master</span> <span class="n">k8s</span><span class="o">:</span><span class="c1">//https://&lt;k8s-apiserver-host&gt;:&lt;k8s-apiserver-port&gt; \</span>
<span class="c1">    --deploy-mode cluster \</span>
<span class="c1">    --name spark-pi \</span>
<span class="c1">    --class org.apache.spark.examples.SparkPi \</span>
<span class="c1">    --jars https://path/to/dependency1.jar,https://path/to/dependency2.jar</span>
    <span class="o">--</span><span class="n">files</span> <span class="n">hdfs</span><span class="o">:</span><span class="c1">//host:port/path/to/file1,hdfs://host:port/path/to/file2</span>
    <span class="o">--</span><span class="n">conf</span> <span class="n">spark</span><span class="p">.</span><span class="n">executor</span><span class="p">.</span><span class="n">instances</span><span class="o">=</span><span class="mi">5</span> \
    <span class="o">--</span><span class="n">conf</span> <span class="n">spark</span><span class="p">.</span><span class="n">kubernetes</span><span class="p">.</span><span class="n">container</span><span class="p">.</span><span class="n">image</span><span class="o">=&lt;</span><span class="n">spark</span><span class="o">-</span><span class="n">image</span><span class="o">&gt;</span> \
    <span class="nl">https:</span><span class="c1">//path/to/examples.jar</span>
</pre></div>


<p>其中，Spark master是Kubernetes api server的地址，可以通过以下命令获取：</p>
<div class="hlcode"><pre><span class="err">$</span> <span class="n">kubectl</span> <span class="n">cluster</span><span class="o">-</span><span class="n">info</span>
<span class="n">Kubernetes</span> <span class="n">master</span> <span class="n">is</span> <span class="n">running</span> <span class="n">at</span> <span class="n">http</span><span class="o">:</span><span class="c1">//127.0.0.1:6443</span>
</pre></div>


<p>Spark的作业代码和依赖，我们可以在<code>--jars</code>、<code>--files</code>和最后位置指定，协议支持http、https和hdfs。</p>
<h2 id="spark-driver-ui">Spark Driver UI</h2>
<p>可以在本地使用<code>kubectl port-forward</code>访问Driver UI：</p>
<div class="hlcode"><pre><span class="err">$</span> <span class="n">kubectl</span> <span class="n">port</span><span class="o">-</span><span class="n">forward</span> <span class="o">&lt;</span><span class="n">driver</span><span class="o">-</span><span class="n">pod</span><span class="o">-</span><span class="n">name</span><span class="o">&gt;</span> <span class="mi">4040</span><span class="o">:</span><span class="mi">4040</span>
</pre></div>


<p>执行完后通过http://localhost:4040访问。</p>
<h2 id="_9">访问日志</h2>
<p>Spark的所有日志都可以通过Kubernetes API和kubectl CLI进行访问：</p>
<div class="hlcode"><pre><span class="err">$</span> <span class="nx">kubectl</span> <span class="na">-n</span><span class="o">=&lt;</span><span class="nx">namespace</span><span class="o">&gt;</span> <span class="nx">logs</span> <span class="na">-f</span> <span class="o">&lt;</span><span class="nx">driver</span><span class="na">-pod-name</span><span class="o">&gt;</span>
</pre></div>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2020 Xu XueHua.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2020-12-12 14:54:32</p>
      </span>
    </div>

    
    
  </body>
</html>