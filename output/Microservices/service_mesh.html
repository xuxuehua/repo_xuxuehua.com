<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/static/css/tango.css">
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>service_mesh - Xu XueHua</title>
    <meta name="keywords" content="Xu XueHua"/>
    <meta name="description" content="Xu XueHua's public notes"/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
    <div id="container">
      
<div id="header">
  <div class="post-nav"><a href="/">Home</a>&nbsp;&#187;&nbsp;<a href="/#Microservices">Microservices</a>&nbsp;&#187;&nbsp;service_mesh
    <span class="updated">Page Updated&nbsp;
      2020-11-28 17:53
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">service_mesh</div>

  <p>[toc]</p>
<h1 id="service-mesh">Service Mesh</h1>
<p>CNCF（Cloud Native Computing Foundation，云原生计算基金会）目前主力推动的新一代的微服务架构——Service Mesh 服务网格。</p>
<p>说白了，就是下面几个特点。</p>
<ul>
<li>Service Mesh 是一个基础设施。</li>
<li>Service Mesh 是一个轻量的服务通讯的网络代理。</li>
<li>Service Mesh 对于应用服务来说是透明无侵入的。</li>
<li>Service Mesh 用于解耦和分离分布式系统架构中控制层面上的东西。</li>
</ul>
<p>说起来，Service Mesh 就像是网络七层模型中的第四层 TCP 协议。其把底层的那些非常难控制的网络通讯方面的控制面的东西都管了（比如：丢包重传、拥塞控制、流量控制），而更为上面的应用层的协议，只需要关心自己业务应用层上的事了。如 HTTP 的 HTML 协议。</p>
<p>在 <a href="http://philcalcado.com/2017/08/03/pattern_service_mesh.html">Pattern: Service Mesh</a> 这篇文章里也详细解释了 Service Mesh 的出现并不是一个偶然，而是一个必然，其中的演化路径如下。</p>
<ul>
<li>一开始是最原始的两台主机间的进程直接通信。</li>
<li>然后分离出网络层来，服务间的远程通信，通过底层的网络模型完成。</li>
<li>再后来，因为两边的服务在接收的速度上不一致，所以需要应用层中实现流控。</li>
<li>后来发现，流控模块基本可以交给网络层实现，于是 TCP/IP 就成了世界上最成功的网络协议。</li>
<li>再往后面，我们知道了分布式系统中的 8 个谬论 <a href="https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing">The 8 Fallacies of Distributed Computing</a> ，意识到需要在分布式系统中有 " 弹力设计 "。于是，我们在更上层中加入了像限流、熔断、服务发现、监控等功能。</li>
<li>然后，我们发现这些弹力设计的模式都是可以标准化的。将这些模式写成 SDK/Lib/Framework，这样就可以在开发层面上很容易地集成到我们的应用服务中。</li>
<li>接下来，我们发现，SDK、Lib、Framework 不能跨编程语言。有什么改动后，要重新编译重新发布服务，太不方便了。应该有一个专门的层来干这事，于是出现了 Sidecar。</li>
</ul>
<p><img alt="img" src="service_mesh.assets/d8aaf6cfe490ffc3b89d08decf7c96c7.png" /><br />
图片来自<a href="http://philcalcado.com/2017/08/03/pattern_service_mesh.html">Pattern: Service Mesh</a></p>
<p>然后呢，Sidecar 集群就成了 Service Mesh。图中的绿色模块是真实的业务应用服务，蓝色模块则是 Sidecar，其组成了一个网格。而我们的应用服务完全独立自包含，只需要和本机的 Sidecar 依赖，剩下的事全交给了 Sidecar。</p>
<p><img alt="img" src="service_mesh.assets/e9235eeaf30df456748d391144bd2bbd.png" /><br />
图片来自<a href="http://philcalcado.com/2017/08/03/pattern_service_mesh.html">Pattern: Service Mesh</a></p>
<p>于是 Sidecar 组成了一个平台，一个 Cloud Native 的服务流量调度的平台（你是否还记得我在《分布式系统的本质》那一系列文章中所说的关键技术中的流量调度和应用监控，其都可以通过 Service Mesh 这个平台来完成）。</p>
<p><img alt="img" src="service_mesh.assets/3d66848ecdc7e582015d8178e702d3d1.png" /><br />
图片来自<a href="http://philcalcado.com/2017/08/03/pattern_service_mesh.html">Pattern: Service Mesh</a></p>
<p>加上对整个集群的管理控制面板，就成了我们整个的 Service Mesh 架构。</p>
<p><img alt="img" src="service_mesh.assets/bf90978e3488ff0c8eb5f8c759ab1078.png" /><br />
图片来自<a href="http://philcalcado.com/2017/08/03/pattern_service_mesh.html">Pattern: Service Mesh</a></p>
<p><img alt="img" src="service_mesh.assets/bb846cf73db84f1551f3051fc1705b3f.png" /><br />
图片来自<a href="http://philcalcado.com/2017/08/03/pattern_service_mesh.html">Pattern: Service Mesh</a></p>
<h1 id="service-mesh_1">Service Mesh 相关的开源软件</h1>
<p>目前比较流行的 Service Mesh 开源软件是 <a href="https://istio.io/">Istio</a> 和 <a href="https://linkerd.io/">Linkerd</a>，它们都可以在 Kubernetes 中集成。当然，还有一个新成员 <a href="https://conduit.io/">Conduit</a>，它是由 Linkerd 的作者出来自己搞的，由 Rust 和 Go 写成的。Rust 负责数据层面，Go 负责控制面。号称吸取了很多 Linkerd 的 Scala 的教训，比 Linkerd 更快，还轻，更简单。</p>
<p>我虽然不是语言的偏好者，但是，不可否认 Rust/Go 的性能方面比 Scala 要好得多得多，尤其是要做成一个和网络通讯相关的基础设施，性能是比较重要的。</p>
<p>对此，我还是推荐大家使用 Rust/Go 语言实现的 lstio 和 Conduit，后者比前者要轻很多。你可以根据你的具体需求挑选，或是自己实现。</p>
<p>lstio 是目前最主流的解决方案，其架构并不复杂，其核心的 Sidecar 被叫做 Envoy（使者），用来协调服务网格中所有服务的出入站流量，并提供服务发现、负载均衡、限流熔断等能力，还可以收集大量与流量相关的性能指标。</p>
<p>在 Service Mesh 控制面上，有一个叫 Mixer 的收集器，用来从 Envoy 收集相关的被监控到的流量特征和性能指标。然后，通过 Pilot 的控制器将相关的规则发送到 Envoy 中，让 Envoy 应用新的规则。</p>
<p>最后，还有一个为安全设计的 lstio-Auth 身份认证组件，用来做服务间的访问安全控制。</p>
<p>整个 lstio 的架构图如下。</p>
<p><img alt="img" src="service_mesh.assets/1a579db1c95608588052b167e68836f2.png" /></p>
<h1 id="service-mesh_2">Service Mesh 的设计重点</h1>
<p>Service Mesh 作为 Sidecar 一个集群应用，Sidecar 需要的微观层面上的那些设计要点在这里就不再复述了，欢迎大家看我之前的文章。这里，更多地说一下 Service Mesh 在整体架构上的一些设计要点。</p>
<p>我们知道，像 Kubernetes 和 Docker 也是分布式系统管理面上的技术解决方案，它们一样对于应用程序是透明的。最重要的是，Kubernetes 和 Docker 对于应用服务的干扰是比较少的。也就是说，Kubernetes 和 Docker 的服务进程的失败不会导致应用服务的异常运行。然后，Service Mesh 则不是，因为其调度了流量，所以，如果 Service Mesh 有 bug，或是 Sidecar 的组件不可用，就会导致整个架构出现致命的问题。</p>
<p>所以，在设计 Service Mesh 的时候，我们需要小心考虑，如果 Service Mesh 所管理的 Sidecar 出了问题，那应该怎么办？所以，Service Mesh 这个网格一定要是高可靠的，或者是出现了故障有 workaround 的方式。一种比较好的方式是，除了在本机有 Sidecar，我们还可以部署一下稍微集中一点的 Sidecar——比如为某个服务集群部署一个集中式的 Sidecar。一旦本机的有问题，可以走集中的。</p>
<p>这样一来，Sidecar 本来就是用来调度流量的，而且其粒度可以细到每个服务的实例，可以粗到一组服务，还可以粗到整体接入。这看来看去都像是一个 Gateway 的事。所以，我相信，使用 Gateway 来干这个事应该是最合适不过的了。这样，我们的 Service Mesh 的想像空间一下子就大多了。</p>
<p>Service Mesh 不像 Sidecar 需要和 Service 一起打包一起部署，Service Mesh 完全独立部署。这样一来，Service Mesh 就成了一个基础设施，就像一个 PaaS 平台。所以，Service Mesh 能不能和 Kubernetes 密切结合就成为了非常关键的因素。</p>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2020 Xu XueHua.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2020-12-12 14:54:32</p>
      </span>
    </div>

    
    
  </body>
</html>